{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2377,"status":"ok","timestamp":1768368985747,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"},"user_tz":480},"id":"DbWv6fHSPIKN","outputId":"93113607-e582-4313-ca08-12092f7a749c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[Errno 2] No such file or directory: 'drive/My Drive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials'\n","/content\n","total 20\n","drwxr-xr-x 1 root root 4096 Jan 14 05:36 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root 4096 Jan 14 05:09 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 4 root root 4096 Dec  9 14:41 \u001b[01;34m.config\u001b[0m/\n","drwx------ 6 root root 4096 Jan 14 05:36 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 1 root root 4096 Dec  9 14:42 \u001b[01;34msample_data\u001b[0m/\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","FOLDERNAME = 'MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd drive/My\\ Drive/$FOLDERNAME\n","%ls -la"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2677,"status":"ok","timestamp":1732596017655,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"},"user_tz":480},"id":"jN2zPySDPU50","outputId":"8d96df5f-5ea6-4ee9-ff60-5a5ec67c28f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0.2)\n"]}],"source":["!pip install PyYAML"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732596017655,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"},"user_tz":480},"id":"mNuCa3cyQJxx","outputId":"668c5526-b0c0-42f5-879b-79b82c0fccac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded configuration:\n","channels:\n","- defaults\n","- conda-forge\n","- pytorch\n","dependencies:\n","- python=3.8\n","- numpy=1.24.3\n","- scipy=1.10.1\n","- scikit-learn=1.3.0\n","- matplotlib=3.7.2\n","- tqdm=4.66.5\n","- docopt=0.6.2\n","- notebook=7.0.8\n","- pip=24.2\n","- tensorboard\n","- pip:\n","  - sentencepiece==0.2.0\n","  - sacrebleu==2.4.3\n","  - nltk==3.9.1\n","  - timeout_decorator==0.5.0\n","  - --find-links https://download.pytorch.org/whl/torch/\n","  - torch==2.4.1+cu118\n","name: XCS224N_CUDA\n","\n","\n","Accessing specific values (adjust these based on your YAML structure):\n"]}],"source":["import yaml\n","import os\n","# Function to load YAML configuration\n","def load_config(config_path):\n","    with open(config_path, 'r') as file:\n","        return yaml.safe_load(file)\n","\n","# Path to your YAML file\n","yaml_path = './src/environment_cuda.yml'\n","\n","# Check if the file exists\n","if not os.path.exists(yaml_path):\n","    print(f\"Error: The file {yaml_path} does not exist.\")\n","else:\n","    try:\n","        # Load the configuration\n","        config = load_config(yaml_path)\n","\n","        # Print the loaded configuration\n","        print(\"Loaded configuration:\")\n","        print(yaml.dump(config, default_flow_style=False))\n","\n","        # Access configuration values (adjust these based on your actual YAML structure)\n","        print(\"\\nAccessing specific values (adjust these based on your YAML structure):\")\n","        if 'database' in config:\n","            print(f\"Database host: {config['database'].get('host', 'Not specified')}\")\n","        if 'api' in config:\n","            print(f\"API URL: {config['api'].get('url', 'Not specified')}\")\n","        if 'logging' in config:\n","            print(f\"Logging level: {config['logging'].get('level', 'Not specified')}\")\n","        if 'features' in config:\n","            print(f\"Feature1 enabled: {config['features'].get('feature1', 'Not specified')}\")\n","\n","        # Example of modifying a configuration value\n","        # Uncomment and adjust these lines if you want to modify the config\n","        # config['database']['port'] = 5433\n","\n","        # Save the modified configuration\n","        # Uncomment these lines if you've made modifications and want to save them\n","        # with open('config_modified.yaml', 'w') as file:\n","        #     yaml.dump(config, file, default_flow_style=False)\n","        # print(\"\\nModified configuration saved to 'config_modified.yaml'\")\n","\n","    except yaml.YAMLError as e:\n","        print(f\"Error reading the YAML file: {e}\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bk4e1hbv1a1W"},"outputs":[],"source":["import os\n","import yaml\n","\n","with open(yaml_path) as file_handle:\n","    environment_data = yaml.safe_load(file_handle)\n","\n","for dependency in environment_data[\"dependencies\"]:\n","    if isinstance(dependency, dict):\n","      for lib in dependency['pip']:\n","        os.system(f\"pip install {lib}\")"]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XdYpCcXVwgrM","executionInfo":{"status":"ok","timestamp":1732595995759,"user_tz":480,"elapsed":177809,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"7f00c0c1-af0f-423b-c684-3116bc2061f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch/\n","Collecting numpy==1.24.3 (from -r requirements.txt (line 1))\n","  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting scipy==1.10.1 (from -r requirements.txt (line 2))\n","  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn==1.3.0 (from -r requirements.txt (line 3))\n","  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting matplotlib==3.7.2 (from -r requirements.txt (line 4))\n","  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting tqdm==4.66.5 (from -r requirements.txt (line 5))\n","  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docopt==0.6.2 (from -r requirements.txt (line 6))\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting notebook==7.0.8 (from -r requirements.txt (line 7))\n","  Downloading notebook-7.0.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.17.1)\n","Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\n","Requirement already satisfied: sacrebleu==2.4.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.4.3)\n","Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.9.1)\n","Requirement already satisfied: timeout_decorator==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.5.0)\n","Collecting torch==2.4.1+cu118 (from -r requirements.txt (line 14))\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.1%2Bcu118-cp310-cp310-linux_x86_64.whl (857.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.6/857.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 3)) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 3)) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (11.0.0)\n","Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->-r requirements.txt (line 4))\n","  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (2.8.2)\n","Collecting jupyter-server<3,>=2.4.0 (from notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n","Collecting jupyterlab-server<3,>=2.22.1 (from notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jupyterlab<4.1,>=4.0.2 (from notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyterlab-4.0.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.8->-r requirements.txt (line 7)) (0.2.4)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.8->-r requirements.txt (line 7)) (6.3.3)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.4.3->-r requirements.txt (line 10)) (3.0.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.4.3->-r requirements.txt (line 10)) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.4.3->-r requirements.txt (line 10)) (0.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.4.3->-r requirements.txt (line 10)) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.4.3->-r requirements.txt (line 10)) (5.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1->-r requirements.txt (line 11)) (8.1.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1+cu118->-r requirements.txt (line 14)) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.3.0.86 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-nccl-cu11==2.20.5 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.8.86 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.0.0 (from torch==2.4.1+cu118->-r requirements.txt (line 14))\n","  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.68.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (4.25.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.3)\n","Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (3.7.1)\n","Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (23.1.0)\n","Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (5.7.2)\n","Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (7.16.4)\n","Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (5.10.4)\n","Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.21.0)\n","Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (24.0.1)\n","Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.18.1)\n","Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (5.7.1)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.8.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1+cu118->-r requirements.txt (line 14)) (3.0.2)\n","Collecting async-lru>=1.0.0 (from jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (5.5.6)\n","Collecting jupyter-lsp>=2.0.0 (from jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (2.1.0)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (2.16.0)\n","Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading json5-0.9.28-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (4.23.0)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (2.32.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1+cu118->-r requirements.txt (line 14)) (1.3.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.2.2)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (21.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (0.21.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (4.3.6)\n","Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (6.0.2)\n","Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (4.12.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (3.0.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.5.1)\n","Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (2.18.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.4.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (2.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook==7.0.8->-r requirements.txt (line 7)) (2024.8.30)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.2.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (7.34.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (0.5.1)\n","Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (3.0.48)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (4.9.0)\n","Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (3.0.0)\n","Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (24.11.1)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7)) (2.22)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.8.4)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyterlab<4.1,>=4.0.2->notebook==7.0.8->-r requirements.txt (line 7)) (0.2.13)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.0.8->-r requirements.txt (line 7))\n","  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n","Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading notebook-7.0.8-py3-none-any.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (23.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl (58.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl (128.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl (204.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab-4.0.13-py3-none-any.whl (9.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n","Downloading json5-0.9.28-py3-none-any.whl (30 kB)\n","Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n","Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n","Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n","Building wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=be98f5d6dc07bf725e3cda741d24341b77a15ffe1a45ff0613ecacc88bebef56\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built docopt\n","Installing collected packages: docopt, uri-template, types-python-dateutil, triton, tqdm, rfc3986-validator, rfc3339-validator, python-json-logger, pyparsing, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, json5, jedi, fqdn, async-lru, scipy, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jupyter-server-terminals, jupyter-client, arrow, torch, scikit-learn, matplotlib, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.6\n","    Uninstalling tqdm-4.66.6:\n","      Successfully uninstalled tqdm-4.66.6\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.2.0\n","    Uninstalling pyparsing-3.2.0:\n","      Successfully uninstalled pyparsing-3.2.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.5.2\n","    Uninstalling scikit-learn-1.5.2:\n","      Successfully uninstalled scikit-learn-1.5.2\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.8.0\n","    Uninstalling matplotlib-3.8.0:\n","      Successfully uninstalled matplotlib-3.8.0\n","  Attempting uninstall: jupyter-server\n","    Found existing installation: jupyter-server 1.24.0\n","    Uninstalling jupyter-server-1.24.0:\n","      Successfully uninstalled jupyter-server-1.24.0\n","  Attempting uninstall: notebook\n","    Found existing installation: notebook 6.5.5\n","    Uninstalling notebook-6.5.5:\n","      Successfully uninstalled notebook-6.5.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 7.0.8 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\n","plotnine 0.14.1 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.1+cu118 which is incompatible.\n","torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.4 docopt-0.6.2 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.9.28 jupyter-client-8.6.3 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.2 jupyter-server-terminals-0.5.3 jupyterlab-4.0.13 jupyterlab-server-2.27.3 matplotlib-3.7.2 notebook-7.0.8 numpy-1.24.3 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 overrides-7.7.0 pyparsing-3.0.9 python-json-logger-2.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scikit-learn-1.3.0 scipy-1.10.1 torch-2.4.1+cu118 tqdm-4.66.5 triton-3.0.0 types-python-dateutil-2.9.0.20241003 uri-template-1.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy","tqdm"]},"id":"9cf8e72f0d3c4c639217f6799d4fda7c"}},"metadata":{}}]},{"cell_type":"code","source":["!cd src/submission/ && python dataset.py charcorruption"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yn7C4LC3eMr9","executionInfo":{"status":"ok","timestamp":1732596028729,"user_tz":480,"elapsed":2463,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"1308fda2-8f0f-42ae-b85d-659bfa09653d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data has 418351 characters, 256 unique.\n","x: Khatchig Mouradian. K⁇a journa⁇hatchig Mouradian is ⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: hatchig Mouradian. K⁇a journa⁇hatchig Mouradian is ⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Jacob Henry Studer. Jaco⁇r⁇b Henry Studer (26 Feb⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: acob Henry Studer. Jaco⁇r⁇b Henry Studer (26 Feb⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: John Stephe⁇n in Glasg⁇n. Bor⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: ohn Stephe⁇n in Glasg⁇n. Bor⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Georgina W⁇llis. Georgina Wil⁇i⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: eorgina W⁇llis. Georgina Wil⁇i⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_finetune_without_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJWa7vMbvkCa","executionInfo":{"status":"ok","timestamp":1732596098381,"user_tz":480,"elapsed":65450,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"919084b4-b9ad-4f94-b0cb-599ffae68b8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 04:40:35.041994: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 04:40:35.060594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 04:40:35.082238: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 04:40:35.088736: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 04:40:35.104181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 04:40:36.267415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","epoch 1 iter 7: train loss 5.52827. lr 5.999844e-04: 100% 8/8 [00:01<00:00,  7.58it/s]\n","epoch 2 iter 7: train loss 2.68931. lr 5.999351e-04: 100% 8/8 [00:00<00:00, 10.66it/s]\n","epoch 3 iter 7: train loss 2.37333. lr 5.998520e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 4 iter 7: train loss 2.19735. lr 5.997351e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 5 iter 7: train loss 2.07665. lr 5.995844e-04: 100% 8/8 [00:00<00:00, 10.67it/s]\n","epoch 6 iter 7: train loss 2.00994. lr 5.993999e-04: 100% 8/8 [00:00<00:00, 10.61it/s]\n","epoch 7 iter 7: train loss 1.94593. lr 5.991818e-04: 100% 8/8 [00:00<00:00, 10.64it/s]\n","epoch 8 iter 7: train loss 1.90632. lr 5.989299e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 9 iter 7: train loss 1.84781. lr 5.986444e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 10 iter 7: train loss 1.77734. lr 5.983252e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 11 iter 7: train loss 1.70977. lr 5.979723e-04: 100% 8/8 [00:00<00:00, 10.86it/s]\n","epoch 12 iter 7: train loss 1.63494. lr 5.975860e-04: 100% 8/8 [00:00<00:00, 10.89it/s]\n","epoch 13 iter 7: train loss 1.58391. lr 5.971660e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n","epoch 14 iter 7: train loss 1.53524. lr 5.967127e-04: 100% 8/8 [00:00<00:00, 10.68it/s]\n","epoch 15 iter 7: train loss 1.47571. lr 5.962258e-04: 100% 8/8 [00:00<00:00,  8.34it/s]\n","epoch 16 iter 7: train loss 1.40819. lr 5.957056e-04: 100% 8/8 [00:00<00:00, 10.54it/s]\n","epoch 17 iter 7: train loss 1.31254. lr 5.951521e-04: 100% 8/8 [00:00<00:00, 10.74it/s]\n","epoch 18 iter 7: train loss 1.24686. lr 5.945654e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n","epoch 19 iter 7: train loss 1.21874. lr 5.939454e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 20 iter 7: train loss 1.12302. lr 5.932923e-04: 100% 8/8 [00:00<00:00, 10.75it/s]\n","epoch 21 iter 7: train loss 1.08031. lr 5.926062e-04: 100% 8/8 [00:00<00:00, 10.72it/s]\n","epoch 22 iter 7: train loss 1.00863. lr 5.918871e-04: 100% 8/8 [00:00<00:00, 10.72it/s]\n","epoch 23 iter 7: train loss 0.96433. lr 5.911352e-04: 100% 8/8 [00:00<00:00, 10.72it/s]\n","epoch 24 iter 7: train loss 0.93249. lr 5.903504e-04: 100% 8/8 [00:00<00:00, 10.50it/s]\n","epoch 25 iter 7: train loss 0.90138. lr 5.895329e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 26 iter 7: train loss 0.85365. lr 5.886828e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 27 iter 7: train loss 0.84337. lr 5.878002e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 28 iter 7: train loss 0.82125. lr 5.868851e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n","epoch 29 iter 7: train loss 0.78615. lr 5.859378e-04: 100% 8/8 [00:00<00:00, 10.75it/s]\n","epoch 30 iter 7: train loss 0.75887. lr 5.849582e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 31 iter 7: train loss 0.73574. lr 5.839465e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 32 iter 7: train loss 0.73210. lr 5.829028e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 33 iter 7: train loss 0.70913. lr 5.818272e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 34 iter 7: train loss 0.70243. lr 5.807199e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 35 iter 7: train loss 0.68116. lr 5.795810e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 36 iter 7: train loss 0.66934. lr 5.784106e-04: 100% 8/8 [00:00<00:00, 10.74it/s]\n","epoch 37 iter 7: train loss 0.65837. lr 5.772087e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n","epoch 38 iter 7: train loss 0.64547. lr 5.759757e-04: 100% 8/8 [00:00<00:00, 10.69it/s]\n","epoch 39 iter 7: train loss 0.63510. lr 5.747116e-04: 100% 8/8 [00:00<00:00, 10.70it/s]\n","epoch 40 iter 7: train loss 0.59075. lr 5.734165e-04: 100% 8/8 [00:00<00:00, 10.68it/s]\n","epoch 41 iter 7: train loss 0.58617. lr 5.720906e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 42 iter 7: train loss 0.58204. lr 5.707341e-04: 100% 8/8 [00:00<00:00, 10.70it/s]\n","epoch 43 iter 7: train loss 0.58154. lr 5.693470e-04: 100% 8/8 [00:00<00:00, 10.66it/s]\n","epoch 44 iter 7: train loss 0.56521. lr 5.679296e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 45 iter 7: train loss 0.53582. lr 5.664820e-04: 100% 8/8 [00:00<00:00, 10.77it/s]\n","epoch 46 iter 7: train loss 0.50615. lr 5.650044e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 47 iter 7: train loss 0.51729. lr 5.634970e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 48 iter 7: train loss 0.50411. lr 5.619598e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 49 iter 7: train loss 0.50259. lr 5.603932e-04: 100% 8/8 [00:00<00:00, 10.80it/s]\n","epoch 50 iter 7: train loss 0.47021. lr 5.587972e-04: 100% 8/8 [00:00<00:00, 10.77it/s]\n","epoch 51 iter 7: train loss 0.47366. lr 5.571720e-04: 100% 8/8 [00:00<00:00, 10.74it/s]\n","epoch 52 iter 7: train loss 0.45829. lr 5.555179e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 53 iter 7: train loss 0.46655. lr 5.538350e-04: 100% 8/8 [00:00<00:00, 10.75it/s]\n","epoch 54 iter 7: train loss 0.43727. lr 5.521235e-04: 100% 8/8 [00:00<00:00, 10.78it/s]\n","epoch 55 iter 7: train loss 0.41732. lr 5.503835e-04: 100% 8/8 [00:00<00:00,  9.74it/s]\n","epoch 56 iter 7: train loss 0.41460. lr 5.486154e-04: 100% 8/8 [00:00<00:00, 10.59it/s]\n","epoch 57 iter 7: train loss 0.40285. lr 5.468193e-04: 100% 8/8 [00:00<00:00, 10.67it/s]\n","epoch 58 iter 7: train loss 0.39107. lr 5.449953e-04: 100% 8/8 [00:00<00:00, 10.84it/s]\n","epoch 59 iter 7: train loss 0.36053. lr 5.431438e-04: 100% 8/8 [00:00<00:00, 10.77it/s]\n","epoch 60 iter 7: train loss 0.37574. lr 5.412648e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 61 iter 7: train loss 0.34217. lr 5.393587e-04: 100% 8/8 [00:00<00:00, 10.83it/s]\n","epoch 62 iter 7: train loss 0.34179. lr 5.374256e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 63 iter 7: train loss 0.33132. lr 5.354657e-04: 100% 8/8 [00:00<00:00, 10.80it/s]\n","epoch 64 iter 7: train loss 0.32294. lr 5.334794e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 65 iter 7: train loss 0.30944. lr 5.314667e-04: 100% 8/8 [00:00<00:00, 10.83it/s]\n","epoch 66 iter 7: train loss 0.29900. lr 5.294279e-04: 100% 8/8 [00:00<00:00, 10.83it/s]\n","epoch 67 iter 7: train loss 0.26401. lr 5.273633e-04: 100% 8/8 [00:00<00:00, 10.80it/s]\n","epoch 68 iter 7: train loss 0.26540. lr 5.252731e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 69 iter 7: train loss 0.26142. lr 5.231575e-04: 100% 8/8 [00:00<00:00, 10.78it/s]\n","epoch 70 iter 7: train loss 0.26299. lr 5.210167e-04: 100% 8/8 [00:00<00:00, 10.71it/s]\n","epoch 71 iter 7: train loss 0.23459. lr 5.188511e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 72 iter 7: train loss 0.22175. lr 5.166608e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 73 iter 7: train loss 0.21301. lr 5.144461e-04: 100% 8/8 [00:00<00:00, 10.85it/s]\n","epoch 74 iter 7: train loss 0.18866. lr 5.122072e-04: 100% 8/8 [00:00<00:00, 10.82it/s]\n","epoch 75 iter 7: train loss 0.18072. lr 5.099444e-04: 100% 8/8 [00:00<00:00, 10.89it/s]\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_eval_dev_without_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LK0o_KexrPF","executionInfo":{"status":"ok","timestamp":1732596201240,"user_tz":480,"elapsed":68350,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"35a97162-a742-41b4-d37e-bc56cc4dba78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 04:42:14.824493: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 04:42:14.843183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 04:42:14.864374: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 04:42:14.870714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 04:42:14.885705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 04:42:16.059643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:02,  8.05it/s]\n","Correct: 5.0 out of 500.0: 1.0%\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_eval_test_without_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Gj-bjDJxyzd","executionInfo":{"status":"ok","timestamp":1732596259869,"user_tz":480,"elapsed":58634,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"9bbdf05b-9ec8-43a8-bc36-a8f9efaf4cf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 04:43:23.090529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 04:43:23.109441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 04:43:23.130532: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 04:43:23.136902: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 04:43:23.152131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 04:43:24.313949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:52,  8.34it/s]\n","!!! No ground truth is provided, this will be done on the autograder, returning (0,0)\n","Predictions written to ./submission/vanilla.nopretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVDYYRopylRJ","executionInfo":{"status":"ok","timestamp":1732597201379,"user_tz":480,"elapsed":821697,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"04333ee0-183e-4252-dcbf-d25f2044ada8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Vanilla Pretrain: ~ 2 Hours\n","2024-11-26 04:46:21.681890: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 04:46:21.700872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 04:46:21.722197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 04:46:21.728608: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 04:46:21.744022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 04:46:22.925202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","epoch 1 iter 22: train loss 3.44873. lr 5.999655e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 2 iter 22: train loss 3.15619. lr 5.998582e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 3 iter 22: train loss 3.02412. lr 5.996780e-03: 100% 23/23 [00:01<00:00, 17.25it/s]\n","epoch 4 iter 22: train loss 2.92046. lr 5.994250e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 5 iter 22: train loss 2.85292. lr 5.990993e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 6 iter 22: train loss 2.80633. lr 5.987009e-03: 100% 23/23 [00:01<00:00, 17.95it/s]\n","epoch 7 iter 22: train loss 2.75665. lr 5.982299e-03: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 8 iter 22: train loss 2.73226. lr 5.976865e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 9 iter 22: train loss 2.72376. lr 5.970707e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 10 iter 22: train loss 2.69439. lr 5.963828e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 11 iter 22: train loss 2.68249. lr 5.956228e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 12 iter 22: train loss 2.68856. lr 5.947911e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 13 iter 22: train loss 2.66459. lr 5.938877e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 14 iter 22: train loss 2.62647. lr 5.929129e-03: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 15 iter 22: train loss 2.65649. lr 5.918669e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 16 iter 22: train loss 2.63348. lr 5.907500e-03: 100% 23/23 [00:01<00:00, 18.16it/s]\n","epoch 17 iter 22: train loss 2.61581. lr 5.895625e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 18 iter 22: train loss 2.62048. lr 5.883046e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 19 iter 22: train loss 2.61036. lr 5.869767e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 20 iter 22: train loss 2.61646. lr 5.855791e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 21 iter 22: train loss 2.62625. lr 5.841120e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 22 iter 22: train loss 2.60345. lr 5.825760e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 23 iter 22: train loss 2.58306. lr 5.809713e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 24 iter 22: train loss 2.56388. lr 5.792983e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 25 iter 22: train loss 2.57904. lr 5.775575e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 26 iter 22: train loss 2.56478. lr 5.757492e-03: 100% 23/23 [00:01<00:00, 17.86it/s]\n","epoch 27 iter 22: train loss 2.58739. lr 5.738739e-03: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 28 iter 22: train loss 2.58250. lr 5.719321e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 29 iter 22: train loss 2.55052. lr 5.699242e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 30 iter 22: train loss 2.54285. lr 5.678508e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 31 iter 22: train loss 2.55267. lr 5.657123e-03: 100% 23/23 [00:01<00:00, 18.65it/s]\n","epoch 32 iter 22: train loss 2.51878. lr 5.635092e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 33 iter 22: train loss 2.52315. lr 5.612421e-03: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 34 iter 22: train loss 2.52473. lr 5.589115e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 35 iter 22: train loss 2.53368. lr 5.565180e-03: 100% 23/23 [00:01<00:00, 18.10it/s]\n","epoch 36 iter 22: train loss 2.53431. lr 5.540622e-03: 100% 23/23 [00:01<00:00, 18.03it/s]\n","epoch 37 iter 22: train loss 2.52306. lr 5.515446e-03: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 38 iter 22: train loss 2.50031. lr 5.489660e-03: 100% 23/23 [00:01<00:00, 18.25it/s]\n","epoch 39 iter 22: train loss 2.48698. lr 5.463268e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 40 iter 22: train loss 2.48130. lr 5.436278e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 41 iter 22: train loss 2.45325. lr 5.408696e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 42 iter 22: train loss 2.49108. lr 5.380529e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 43 iter 22: train loss 2.44861. lr 5.351784e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 44 iter 22: train loss 2.46732. lr 5.322467e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 45 iter 22: train loss 2.47202. lr 5.292586e-03: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 46 iter 22: train loss 2.43442. lr 5.262148e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 47 iter 22: train loss 2.39402. lr 5.231160e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 48 iter 22: train loss 2.42854. lr 5.199630e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 49 iter 22: train loss 2.39362. lr 5.167566e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 50 iter 22: train loss 2.38093. lr 5.134975e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 51 iter 22: train loss 2.40333. lr 5.101865e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 52 iter 22: train loss 2.37927. lr 5.068245e-03: 100% 23/23 [00:01<00:00, 17.75it/s]\n","epoch 53 iter 22: train loss 2.32513. lr 5.034122e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 54 iter 22: train loss 2.35107. lr 4.999505e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 55 iter 22: train loss 2.33168. lr 4.964402e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 56 iter 22: train loss 2.33314. lr 4.928822e-03: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 57 iter 22: train loss 2.30137. lr 4.892774e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 58 iter 22: train loss 2.29945. lr 4.856265e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 59 iter 22: train loss 2.27801. lr 4.819305e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 60 iter 22: train loss 2.24471. lr 4.781904e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 61 iter 22: train loss 2.24200. lr 4.744069e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 62 iter 22: train loss 2.19781. lr 4.705811e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 63 iter 22: train loss 2.17417. lr 4.667138e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 64 iter 22: train loss 2.18945. lr 4.628061e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 65 iter 22: train loss 2.14095. lr 4.588587e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 66 iter 22: train loss 2.09141. lr 4.548728e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 67 iter 22: train loss 2.07982. lr 4.508492e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 68 iter 22: train loss 2.06296. lr 4.467890e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 69 iter 22: train loss 2.13234. lr 4.426932e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 70 iter 22: train loss 2.05451. lr 4.385626e-03: 100% 23/23 [00:01<00:00, 18.79it/s]\n","epoch 71 iter 22: train loss 2.06520. lr 4.343984e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 72 iter 22: train loss 2.04008. lr 4.302016e-03: 100% 23/23 [00:01<00:00, 18.72it/s]\n","epoch 73 iter 22: train loss 2.02005. lr 4.259731e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 74 iter 22: train loss 1.98741. lr 4.217140e-03: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 75 iter 22: train loss 1.94918. lr 4.174253e-03: 100% 23/23 [00:01<00:00, 18.17it/s]\n","epoch 76 iter 22: train loss 1.95074. lr 4.131081e-03: 100% 23/23 [00:01<00:00, 18.05it/s]\n","epoch 77 iter 22: train loss 1.92305. lr 4.087634e-03: 100% 23/23 [00:01<00:00, 18.13it/s]\n","epoch 78 iter 22: train loss 1.94810. lr 4.043923e-03: 100% 23/23 [00:01<00:00, 18.15it/s]\n","epoch 79 iter 22: train loss 1.89303. lr 3.999958e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 80 iter 22: train loss 1.87236. lr 3.955751e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 81 iter 22: train loss 1.92516. lr 3.911311e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 82 iter 22: train loss 1.89511. lr 3.866650e-03: 100% 23/23 [00:01<00:00, 17.80it/s]\n","epoch 83 iter 22: train loss 1.84635. lr 3.821778e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 84 iter 22: train loss 1.88341. lr 3.776706e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 85 iter 22: train loss 1.84167. lr 3.731446e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 86 iter 22: train loss 1.80289. lr 3.686008e-03: 100% 23/23 [00:01<00:00, 18.07it/s]\n","epoch 87 iter 22: train loss 1.79822. lr 3.640404e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 88 iter 22: train loss 1.78440. lr 3.594643e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 89 iter 22: train loss 1.75801. lr 3.548739e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 90 iter 22: train loss 1.76356. lr 3.502701e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 91 iter 22: train loss 1.72951. lr 3.456541e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 92 iter 22: train loss 1.76679. lr 3.410270e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 93 iter 22: train loss 1.73402. lr 3.363899e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 94 iter 22: train loss 1.67104. lr 3.317440e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 95 iter 22: train loss 1.62105. lr 3.270903e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 96 iter 22: train loss 1.69937. lr 3.224301e-03: 100% 23/23 [00:01<00:00, 18.04it/s]\n","epoch 97 iter 22: train loss 1.64453. lr 3.177645e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 98 iter 22: train loss 1.62212. lr 3.130945e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 99 iter 22: train loss 1.66864. lr 3.084213e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 100 iter 22: train loss 1.61734. lr 3.037461e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 101 iter 22: train loss 1.63740. lr 2.990700e-03: 100% 23/23 [00:01<00:00, 17.48it/s]\n","epoch 102 iter 22: train loss 1.63867. lr 2.943941e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 103 iter 22: train loss 1.59179. lr 2.897196e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 104 iter 22: train loss 1.56448. lr 2.850476e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 105 iter 22: train loss 1.56202. lr 2.803792e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 106 iter 22: train loss 1.56075. lr 2.757156e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 107 iter 22: train loss 1.55888. lr 2.710579e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 108 iter 22: train loss 1.52460. lr 2.664072e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 109 iter 22: train loss 1.58393. lr 2.617646e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 110 iter 22: train loss 1.52904. lr 2.571314e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 111 iter 22: train loss 1.49949. lr 2.525086e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 112 iter 22: train loss 1.48600. lr 2.478973e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 113 iter 22: train loss 1.48818. lr 2.432986e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 114 iter 22: train loss 1.48901. lr 2.387138e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 115 iter 22: train loss 1.46743. lr 2.341438e-03: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 116 iter 22: train loss 1.50031. lr 2.295899e-03: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 117 iter 22: train loss 1.44271. lr 2.250530e-03: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 118 iter 22: train loss 1.43918. lr 2.205344e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 119 iter 22: train loss 1.44909. lr 2.160350e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 120 iter 22: train loss 1.44633. lr 2.115561e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 121 iter 22: train loss 1.44315. lr 2.070986e-03: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 122 iter 22: train loss 1.43560. lr 2.026638e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 123 iter 22: train loss 1.44942. lr 1.982525e-03: 100% 23/23 [00:01<00:00, 18.76it/s]\n","epoch 124 iter 22: train loss 1.39815. lr 1.938660e-03: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 125 iter 22: train loss 1.43705. lr 1.895053e-03: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 126 iter 22: train loss 1.38314. lr 1.851714e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 127 iter 22: train loss 1.39279. lr 1.808654e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 128 iter 22: train loss 1.38273. lr 1.765884e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 129 iter 22: train loss 1.36495. lr 1.723414e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 130 iter 22: train loss 1.38534. lr 1.681253e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 131 iter 22: train loss 1.36999. lr 1.639413e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 132 iter 22: train loss 1.33669. lr 1.597904e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 133 iter 22: train loss 1.37335. lr 1.556735e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 134 iter 22: train loss 1.36982. lr 1.515917e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 135 iter 22: train loss 1.33163. lr 1.475460e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 136 iter 22: train loss 1.33011. lr 1.435373e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 137 iter 22: train loss 1.30966. lr 1.395666e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 138 iter 22: train loss 1.31348. lr 1.356349e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 139 iter 22: train loss 1.32890. lr 1.317431e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 140 iter 22: train loss 1.34828. lr 1.278922e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 141 iter 22: train loss 1.31782. lr 1.240831e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 142 iter 22: train loss 1.29889. lr 1.203168e-03: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 143 iter 22: train loss 1.32550. lr 1.165941e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 144 iter 22: train loss 1.29421. lr 1.129159e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 145 iter 22: train loss 1.29225. lr 1.092833e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 146 iter 22: train loss 1.29926. lr 1.056969e-03: 100% 23/23 [00:01<00:00, 17.91it/s]\n","epoch 147 iter 22: train loss 1.31670. lr 1.021578e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 148 iter 22: train loss 1.26145. lr 9.866675e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 149 iter 22: train loss 1.28896. lr 9.522460e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 150 iter 22: train loss 1.28606. lr 9.183221e-04: 100% 23/23 [00:01<00:00, 17.97it/s]\n","epoch 151 iter 22: train loss 1.25361. lr 8.849040e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 152 iter 22: train loss 1.28717. lr 8.519997e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 153 iter 22: train loss 1.25733. lr 8.196174e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 154 iter 22: train loss 1.21278. lr 7.877647e-04: 100% 23/23 [00:01<00:00, 18.10it/s]\n","epoch 155 iter 22: train loss 1.23714. lr 7.564496e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 156 iter 22: train loss 1.25460. lr 7.256796e-04: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 157 iter 22: train loss 1.24161. lr 6.954621e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 158 iter 22: train loss 1.20835. lr 6.658045e-04: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 159 iter 22: train loss 1.23532. lr 6.367141e-04: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 160 iter 22: train loss 1.23421. lr 6.081979e-04: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 161 iter 22: train loss 1.20576. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 162 iter 22: train loss 1.22000. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 163 iter 22: train loss 1.25400. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 164 iter 22: train loss 1.19472. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 165 iter 22: train loss 1.25598. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.60it/s]\n","epoch 166 iter 22: train loss 1.23495. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 167 iter 22: train loss 1.21475. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 168 iter 22: train loss 1.20218. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 169 iter 22: train loss 1.22246. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 170 iter 22: train loss 1.21649. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 171 iter 22: train loss 1.17909. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 172 iter 22: train loss 1.19785. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 173 iter 22: train loss 1.20803. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 174 iter 22: train loss 1.19525. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 175 iter 22: train loss 1.17791. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 176 iter 22: train loss 1.19523. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 177 iter 22: train loss 1.19042. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 178 iter 22: train loss 1.21357. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 179 iter 22: train loss 1.18885. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.11it/s]\n","epoch 180 iter 22: train loss 1.20145. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 181 iter 22: train loss 1.21435. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 182 iter 22: train loss 1.23009. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 183 iter 22: train loss 1.17238. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 184 iter 22: train loss 1.19968. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 185 iter 22: train loss 1.17886. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 186 iter 22: train loss 1.18662. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.02it/s]\n","epoch 187 iter 22: train loss 1.17198. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.92it/s]\n","epoch 188 iter 22: train loss 1.20399. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 189 iter 22: train loss 1.20931. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 190 iter 22: train loss 1.19079. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 191 iter 22: train loss 1.17134. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 192 iter 22: train loss 1.12722. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 193 iter 22: train loss 1.18089. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 194 iter 22: train loss 1.17851. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 195 iter 22: train loss 1.17941. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 196 iter 22: train loss 1.16099. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 197 iter 22: train loss 1.16056. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 198 iter 22: train loss 1.18851. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 199 iter 22: train loss 1.17921. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 200 iter 22: train loss 1.17664. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 201 iter 22: train loss 1.13125. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 202 iter 22: train loss 1.15306. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 203 iter 22: train loss 1.14844. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 204 iter 22: train loss 1.15398. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 205 iter 22: train loss 1.15481. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 206 iter 22: train loss 1.17222. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 207 iter 22: train loss 1.12428. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 208 iter 22: train loss 1.18453. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 209 iter 22: train loss 1.16835. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 210 iter 22: train loss 1.12549. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 211 iter 22: train loss 1.12570. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 212 iter 22: train loss 1.17900. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 213 iter 22: train loss 1.13935. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 214 iter 22: train loss 1.17114. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 215 iter 22: train loss 1.16398. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 216 iter 22: train loss 1.14326. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 217 iter 22: train loss 1.13272. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 218 iter 22: train loss 1.17088. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 219 iter 22: train loss 1.15734. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 220 iter 22: train loss 1.14152. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 221 iter 22: train loss 1.10372. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 222 iter 22: train loss 1.14293. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 223 iter 22: train loss 1.13945. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 224 iter 22: train loss 1.12915. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 225 iter 22: train loss 1.15344. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 226 iter 22: train loss 1.16236. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 227 iter 22: train loss 1.12456. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.77it/s]\n","epoch 228 iter 22: train loss 1.16923. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 229 iter 22: train loss 1.13398. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 230 iter 22: train loss 1.11485. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 231 iter 22: train loss 1.11083. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 232 iter 22: train loss 1.12279. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 233 iter 22: train loss 1.11104. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 234 iter 22: train loss 1.12079. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 235 iter 22: train loss 1.08197. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.15it/s]\n","epoch 236 iter 22: train loss 1.12852. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 237 iter 22: train loss 1.10930. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 238 iter 22: train loss 1.14867. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 239 iter 22: train loss 1.11111. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 240 iter 22: train loss 1.11271. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 241 iter 22: train loss 1.09554. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 242 iter 22: train loss 1.10034. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 243 iter 22: train loss 1.13456. lr 6.039815e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 244 iter 22: train loss 1.14985. lr 6.324112e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 245 iter 22: train loss 1.06514. lr 6.614162e-04: 100% 23/23 [00:01<00:00, 18.00it/s]\n","epoch 246 iter 22: train loss 1.10919. lr 6.909893e-04: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 247 iter 22: train loss 1.10088. lr 7.211235e-04: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 248 iter 22: train loss 1.10031. lr 7.518114e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 249 iter 22: train loss 1.12544. lr 7.830454e-04: 100% 23/23 [00:01<00:00, 18.77it/s]\n","epoch 250 iter 22: train loss 1.15085. lr 8.148181e-04: 100% 23/23 [00:01<00:00, 18.77it/s]\n","epoch 251 iter 22: train loss 1.09960. lr 8.471217e-04: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 252 iter 22: train loss 1.11409. lr 8.799484e-04: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 253 iter 22: train loss 1.11507. lr 9.132902e-04: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 254 iter 22: train loss 1.14886. lr 9.471389e-04: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 255 iter 22: train loss 1.11338. lr 9.814865e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 256 iter 22: train loss 1.10074. lr 1.016324e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 257 iter 22: train loss 1.11092. lr 1.051644e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 258 iter 22: train loss 1.10197. lr 1.087438e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 259 iter 22: train loss 1.10385. lr 1.123696e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 260 iter 22: train loss 1.10698. lr 1.160409e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 261 iter 22: train loss 1.15263. lr 1.197570e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 262 iter 22: train loss 1.11192. lr 1.235169e-03: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 263 iter 22: train loss 1.13910. lr 1.273196e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 264 iter 22: train loss 1.11536. lr 1.311643e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 265 iter 22: train loss 1.10938. lr 1.350501e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 266 iter 22: train loss 1.09787. lr 1.389759e-03: 100% 23/23 [00:01<00:00, 18.17it/s]\n","epoch 267 iter 22: train loss 1.10542. lr 1.429408e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 268 iter 22: train loss 1.11743. lr 1.469439e-03: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 269 iter 22: train loss 1.16432. lr 1.509841e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 270 iter 22: train loss 1.08871. lr 1.550606e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 271 iter 22: train loss 1.10024. lr 1.591723e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 272 iter 22: train loss 1.12881. lr 1.633182e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 273 iter 22: train loss 1.10547. lr 1.674973e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 274 iter 22: train loss 1.12311. lr 1.717086e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 275 iter 22: train loss 1.14654. lr 1.759511e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 276 iter 22: train loss 1.11638. lr 1.802237e-03: 100% 23/23 [00:01<00:00, 17.97it/s]\n","epoch 277 iter 22: train loss 1.12991. lr 1.845254e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 278 iter 22: train loss 1.13738. lr 1.888552e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 279 iter 22: train loss 1.16571. lr 1.932120e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 280 iter 22: train loss 1.11807. lr 1.975947e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 281 iter 22: train loss 1.10346. lr 2.020023e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 282 iter 22: train loss 1.10646. lr 2.064337e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 283 iter 22: train loss 1.14815. lr 2.108878e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 284 iter 22: train loss 1.13410. lr 2.153636e-03: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 285 iter 22: train loss 1.08921. lr 2.198600e-03: 100% 23/23 [00:01<00:00, 18.16it/s]\n","epoch 286 iter 22: train loss 1.11169. lr 2.243758e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 287 iter 22: train loss 1.11065. lr 2.289100e-03: 100% 23/23 [00:01<00:00, 18.72it/s]\n","epoch 288 iter 22: train loss 1.14758. lr 2.334615e-03: 100% 23/23 [00:01<00:00, 18.77it/s]\n","epoch 289 iter 22: train loss 1.14012. lr 2.380291e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 290 iter 22: train loss 1.10261. lr 2.426118e-03: 100% 23/23 [00:01<00:00, 18.82it/s]\n","epoch 291 iter 22: train loss 1.12835. lr 2.472085e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 292 iter 22: train loss 1.17833. lr 2.518179e-03: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 293 iter 22: train loss 1.11740. lr 2.564391e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 294 iter 22: train loss 1.11304. lr 2.610708e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 295 iter 22: train loss 1.09176. lr 2.657121e-03: 100% 23/23 [00:01<00:00, 18.11it/s]\n","epoch 296 iter 22: train loss 1.11582. lr 2.703616e-03: 100% 23/23 [00:01<00:00, 18.01it/s]\n","epoch 297 iter 22: train loss 1.12350. lr 2.750184e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 298 iter 22: train loss 1.13462. lr 2.796812e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 299 iter 22: train loss 1.14066. lr 2.843489e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 300 iter 22: train loss 1.15227. lr 2.890205e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 301 iter 22: train loss 1.14179. lr 2.936947e-03: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 302 iter 22: train loss 1.10884. lr 2.983705e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 303 iter 22: train loss 1.12674. lr 3.030466e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 304 iter 22: train loss 1.14094. lr 3.077220e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 305 iter 22: train loss 1.14030. lr 3.123955e-03: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 306 iter 22: train loss 1.09812. lr 3.170661e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 307 iter 22: train loss 1.16588. lr 3.217324e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 308 iter 22: train loss 1.15671. lr 3.263935e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 309 iter 22: train loss 1.11949. lr 3.310482e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 310 iter 22: train loss 1.09490. lr 3.356954e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 311 iter 22: train loss 1.14903. lr 3.403338e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 312 iter 22: train loss 1.12925. lr 3.449625e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 313 iter 22: train loss 1.13194. lr 3.495802e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 314 iter 22: train loss 1.09865. lr 3.541859e-03: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 315 iter 22: train loss 1.13645. lr 3.587785e-03: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 316 iter 22: train loss 1.11087. lr 3.633567e-03: 100% 23/23 [00:01<00:00, 18.04it/s]\n","epoch 317 iter 22: train loss 1.15320. lr 3.679196e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 318 iter 22: train loss 1.16728. lr 3.724659e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 319 iter 22: train loss 1.11138. lr 3.769947e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 320 iter 22: train loss 1.11123. lr 3.815047e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 321 iter 22: train loss 1.09822. lr 3.859950e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 322 iter 22: train loss 1.13750. lr 3.904643e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 323 iter 22: train loss 1.13516. lr 3.949117e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 324 iter 22: train loss 1.09896. lr 3.993360e-03: 100% 23/23 [00:01<00:00, 18.06it/s]\n","epoch 325 iter 22: train loss 1.13284. lr 4.037362e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 326 iter 22: train loss 1.12963. lr 4.081111e-03: 100% 23/23 [00:01<00:00, 18.04it/s]\n","epoch 327 iter 22: train loss 1.10039. lr 4.124598e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 328 iter 22: train loss 1.12554. lr 4.167812e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 329 iter 22: train loss 1.14325. lr 4.210742e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 330 iter 22: train loss 1.16460. lr 4.253378e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 331 iter 22: train loss 1.12149. lr 4.295709e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 332 iter 22: train loss 1.09584. lr 4.337726e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 333 iter 22: train loss 1.10738. lr 4.379418e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 334 iter 22: train loss 1.12733. lr 4.420774e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 335 iter 22: train loss 1.09322. lr 4.461785e-03: 100% 23/23 [00:01<00:00, 18.62it/s]\n","epoch 336 iter 22: train loss 1.11640. lr 4.502441e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 337 iter 22: train loss 1.14635. lr 4.542732e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 338 iter 22: train loss 1.08037. lr 4.582648e-03: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 339 iter 22: train loss 1.09666. lr 4.622180e-03: 100% 23/23 [00:01<00:00, 18.65it/s]\n","epoch 340 iter 22: train loss 1.11335. lr 4.661318e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 341 iter 22: train loss 1.07404. lr 4.700052e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 342 iter 22: train loss 1.10261. lr 4.738372e-03: 100% 23/23 [00:01<00:00, 18.72it/s]\n","epoch 343 iter 22: train loss 1.08850. lr 4.776271e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 344 iter 22: train loss 1.13169. lr 4.813738e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 345 iter 22: train loss 1.10433. lr 4.850764e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 346 iter 22: train loss 1.12575. lr 4.887341e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 347 iter 22: train loss 1.08698. lr 4.923459e-03: 100% 23/23 [00:01<00:00, 17.94it/s]\n","epoch 348 iter 22: train loss 1.10751. lr 4.959110e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 349 iter 22: train loss 1.08073. lr 4.994284e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 350 iter 22: train loss 1.11659. lr 5.028975e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 351 iter 22: train loss 1.07296. lr 5.063172e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 352 iter 22: train loss 1.14253. lr 5.096868e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 353 iter 22: train loss 1.07311. lr 5.130054e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 354 iter 22: train loss 1.09416. lr 5.162723e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 355 iter 22: train loss 1.07910. lr 5.194867e-03: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 356 iter 22: train loss 1.09676. lr 5.226477e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 357 iter 22: train loss 1.08279. lr 5.257546e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 358 iter 22: train loss 1.07919. lr 5.288067e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 359 iter 22: train loss 1.08628. lr 5.318032e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 360 iter 22: train loss 1.10542. lr 5.347434e-03: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 361 iter 22: train loss 1.07762. lr 5.376265e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 362 iter 22: train loss 1.06638. lr 5.404519e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 363 iter 22: train loss 1.06925. lr 5.432189e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 364 iter 22: train loss 1.07306. lr 5.459268e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 365 iter 22: train loss 1.03050. lr 5.485750e-03: 100% 23/23 [00:01<00:00, 18.04it/s]\n","epoch 366 iter 22: train loss 1.06694. lr 5.511627e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 367 iter 22: train loss 1.06594. lr 5.536894e-03: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 368 iter 22: train loss 1.07032. lr 5.561545e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 369 iter 22: train loss 1.08829. lr 5.585574e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 370 iter 22: train loss 1.04279. lr 5.608974e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 371 iter 22: train loss 1.07824. lr 5.631741e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 372 iter 22: train loss 1.09666. lr 5.653868e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 373 iter 22: train loss 1.06886. lr 5.675350e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 374 iter 22: train loss 1.09806. lr 5.696182e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 375 iter 22: train loss 1.06986. lr 5.716359e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 376 iter 22: train loss 1.10479. lr 5.735876e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 377 iter 22: train loss 1.04965. lr 5.754729e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 378 iter 22: train loss 1.05647. lr 5.772912e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 379 iter 22: train loss 1.01767. lr 5.790422e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 380 iter 22: train loss 1.06012. lr 5.807253e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 381 iter 22: train loss 1.03676. lr 5.823403e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 382 iter 22: train loss 1.01431. lr 5.838866e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 383 iter 22: train loss 1.03357. lr 5.853640e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 384 iter 22: train loss 1.04218. lr 5.867720e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 385 iter 22: train loss 1.02504. lr 5.881104e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 386 iter 22: train loss 1.03360. lr 5.893788e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 387 iter 22: train loss 1.04534. lr 5.905768e-03: 100% 23/23 [00:01<00:00, 18.15it/s]\n","epoch 388 iter 22: train loss 1.04508. lr 5.917043e-03: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 389 iter 22: train loss 1.02383. lr 5.927609e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 390 iter 22: train loss 0.96165. lr 5.937464e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 391 iter 22: train loss 1.02162. lr 5.946605e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 392 iter 22: train loss 1.03846. lr 5.955030e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 393 iter 22: train loss 1.02622. lr 5.962737e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 394 iter 22: train loss 1.03886. lr 5.969724e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 395 iter 22: train loss 1.04045. lr 5.975990e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 396 iter 22: train loss 1.03348. lr 5.981532e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 397 iter 22: train loss 0.98715. lr 5.986351e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 398 iter 22: train loss 1.00438. lr 5.990443e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 399 iter 22: train loss 1.02850. lr 5.993809e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 400 iter 22: train loss 0.99168. lr 5.996448e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 401 iter 22: train loss 1.00431. lr 5.998359e-03: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 402 iter 22: train loss 1.04192. lr 5.999541e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 403 iter 22: train loss 1.02740. lr 5.999995e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 404 iter 22: train loss 1.03334. lr 5.999719e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 405 iter 22: train loss 0.98721. lr 5.998715e-03: 100% 23/23 [00:01<00:00, 17.73it/s]\n","epoch 406 iter 22: train loss 1.00945. lr 5.996982e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 407 iter 22: train loss 0.97017. lr 5.994521e-03: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 408 iter 22: train loss 1.02415. lr 5.991333e-03: 100% 23/23 [00:01<00:00, 18.60it/s]\n","epoch 409 iter 22: train loss 1.00995. lr 5.987417e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 410 iter 22: train loss 0.99390. lr 5.982776e-03: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 411 iter 22: train loss 1.01175. lr 5.977411e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 412 iter 22: train loss 0.98376. lr 5.971321e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 413 iter 22: train loss 0.96735. lr 5.964510e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 414 iter 22: train loss 0.98439. lr 5.956979e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 415 iter 22: train loss 0.99169. lr 5.948729e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 416 iter 22: train loss 0.96846. lr 5.939763e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 417 iter 22: train loss 0.98172. lr 5.930082e-03: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 418 iter 22: train loss 0.96824. lr 5.919690e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 419 iter 22: train loss 1.00175. lr 5.908588e-03: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 420 iter 22: train loss 1.00337. lr 5.896780e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 421 iter 22: train loss 1.00575. lr 5.884268e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 422 iter 22: train loss 0.97349. lr 5.871055e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 423 iter 22: train loss 0.98900. lr 5.857144e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 424 iter 22: train loss 0.97126. lr 5.842539e-03: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 425 iter 22: train loss 0.96239. lr 5.827244e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 426 iter 22: train loss 0.96903. lr 5.811262e-03: 100% 23/23 [00:01<00:00, 18.65it/s]\n","epoch 427 iter 22: train loss 0.92101. lr 5.794596e-03: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 428 iter 22: train loss 0.98894. lr 5.777252e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 429 iter 22: train loss 0.95237. lr 5.759233e-03: 100% 23/23 [00:01<00:00, 17.68it/s]\n","epoch 430 iter 22: train loss 0.94706. lr 5.740544e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 431 iter 22: train loss 0.94994. lr 5.721189e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 432 iter 22: train loss 0.95223. lr 5.701172e-03: 100% 23/23 [00:01<00:00, 18.52it/s]\n","epoch 433 iter 22: train loss 0.94091. lr 5.680499e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 434 iter 22: train loss 0.96475. lr 5.659176e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 435 iter 22: train loss 0.91796. lr 5.637206e-03: 100% 23/23 [00:01<00:00, 18.10it/s]\n","epoch 436 iter 22: train loss 0.97377. lr 5.614595e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 437 iter 22: train loss 0.93198. lr 5.591349e-03: 100% 23/23 [00:01<00:00, 17.96it/s]\n","epoch 438 iter 22: train loss 0.93739. lr 5.567473e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 439 iter 22: train loss 0.95391. lr 5.542974e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 440 iter 22: train loss 0.93006. lr 5.517857e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 441 iter 22: train loss 0.91430. lr 5.492128e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 442 iter 22: train loss 0.94961. lr 5.465793e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 443 iter 22: train loss 0.92365. lr 5.438860e-03: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 444 iter 22: train loss 0.92782. lr 5.411333e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 445 iter 22: train loss 0.94021. lr 5.383222e-03: 100% 23/23 [00:01<00:00, 17.46it/s]\n","epoch 446 iter 22: train loss 0.89249. lr 5.354531e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 447 iter 22: train loss 0.91144. lr 5.325267e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 448 iter 22: train loss 0.88197. lr 5.295439e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 449 iter 22: train loss 0.89625. lr 5.265054e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 450 iter 22: train loss 0.89447. lr 5.234118e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 451 iter 22: train loss 0.90062. lr 5.202639e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 452 iter 22: train loss 0.90565. lr 5.170625e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 453 iter 22: train loss 0.86526. lr 5.138084e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 454 iter 22: train loss 0.89606. lr 5.105023e-03: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 455 iter 22: train loss 0.90580. lr 5.071450e-03: 100% 23/23 [00:01<00:00, 17.88it/s]\n","epoch 456 iter 22: train loss 0.87622. lr 5.037375e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 457 iter 22: train loss 0.90625. lr 5.002804e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 458 iter 22: train loss 0.89111. lr 4.967747e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 459 iter 22: train loss 0.87888. lr 4.932212e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 460 iter 22: train loss 0.91045. lr 4.896207e-03: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 461 iter 22: train loss 0.86201. lr 4.859742e-03: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 462 iter 22: train loss 0.86598. lr 4.822825e-03: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 463 iter 22: train loss 0.89030. lr 4.785465e-03: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 464 iter 22: train loss 0.84915. lr 4.747671e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 465 iter 22: train loss 0.87135. lr 4.709452e-03: 100% 23/23 [00:01<00:00, 17.89it/s]\n","epoch 466 iter 22: train loss 0.85192. lr 4.670818e-03: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 467 iter 22: train loss 0.88112. lr 4.631778e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 468 iter 22: train loss 0.85025. lr 4.592342e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 469 iter 22: train loss 0.86060. lr 4.552519e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 470 iter 22: train loss 0.85071. lr 4.512319e-03: 100% 23/23 [00:01<00:00, 18.07it/s]\n","epoch 471 iter 22: train loss 0.84111. lr 4.471751e-03: 100% 23/23 [00:01<00:00, 18.11it/s]\n","epoch 472 iter 22: train loss 0.82953. lr 4.430825e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 473 iter 22: train loss 0.81866. lr 4.389552e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 474 iter 22: train loss 0.85718. lr 4.347942e-03: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 475 iter 22: train loss 0.82638. lr 4.306004e-03: 100% 23/23 [00:01<00:00, 17.98it/s]\n","epoch 476 iter 22: train loss 0.82253. lr 4.263748e-03: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 477 iter 22: train loss 0.79097. lr 4.221186e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 478 iter 22: train loss 0.81687. lr 4.178327e-03: 100% 23/23 [00:01<00:00, 17.96it/s]\n","epoch 479 iter 22: train loss 0.82093. lr 4.135181e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 480 iter 22: train loss 0.79412. lr 4.091760e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 481 iter 22: train loss 0.83372. lr 4.048074e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 482 iter 22: train loss 0.82726. lr 4.004132e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 483 iter 22: train loss 0.82677. lr 3.959947e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 484 iter 22: train loss 0.82964. lr 3.915529e-03: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 485 iter 22: train loss 0.80306. lr 3.870888e-03: 100% 23/23 [00:01<00:00, 17.92it/s]\n","epoch 486 iter 22: train loss 0.79371. lr 3.826036e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 487 iter 22: train loss 0.81155. lr 3.780983e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 488 iter 22: train loss 0.83199. lr 3.735740e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 489 iter 22: train loss 0.82432. lr 3.690318e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 490 iter 22: train loss 0.76634. lr 3.644729e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 491 iter 22: train loss 0.78590. lr 3.598983e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 492 iter 22: train loss 0.76267. lr 3.553092e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 493 iter 22: train loss 0.80578. lr 3.507066e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 494 iter 22: train loss 0.79209. lr 3.460917e-03: 100% 23/23 [00:01<00:00, 17.98it/s]\n","epoch 495 iter 22: train loss 0.81786. lr 3.414656e-03: 100% 23/23 [00:01<00:00, 18.10it/s]\n","epoch 496 iter 22: train loss 0.77312. lr 3.368294e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 497 iter 22: train loss 0.75139. lr 3.321843e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 498 iter 22: train loss 0.77122. lr 3.275313e-03: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 499 iter 22: train loss 0.76398. lr 3.228717e-03: 100% 23/23 [00:01<00:00, 18.51it/s]\n","epoch 500 iter 22: train loss 0.76411. lr 3.182065e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 501 iter 22: train loss 0.76019. lr 3.135369e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 502 iter 22: train loss 0.76535. lr 3.088640e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 503 iter 22: train loss 0.73142. lr 3.041889e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 504 iter 22: train loss 0.76723. lr 2.995129e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 505 iter 22: train loss 0.77636. lr 2.948369e-03: 100% 23/23 [00:01<00:00, 18.07it/s]\n","epoch 506 iter 22: train loss 0.73675. lr 2.901622e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 507 iter 22: train loss 0.74371. lr 2.854899e-03: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 508 iter 22: train loss 0.75768. lr 2.808211e-03: 100% 23/23 [00:01<00:00, 18.67it/s]\n","epoch 509 iter 22: train loss 0.75239. lr 2.761570e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 510 iter 22: train loss 0.72648. lr 2.714987e-03: 100% 23/23 [00:01<00:00, 18.14it/s]\n","epoch 511 iter 22: train loss 0.74135. lr 2.668473e-03: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 512 iter 22: train loss 0.73817. lr 2.622039e-03: 100% 23/23 [00:01<00:00, 18.25it/s]\n","epoch 513 iter 22: train loss 0.76039. lr 2.575697e-03: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 514 iter 22: train loss 0.71669. lr 2.529459e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 515 iter 22: train loss 0.72751. lr 2.483334e-03: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 516 iter 22: train loss 0.70576. lr 2.437336e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 517 iter 22: train loss 0.71254. lr 2.391474e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 518 iter 22: train loss 0.70352. lr 2.345759e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 519 iter 22: train loss 0.73247. lr 2.300204e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 520 iter 22: train loss 0.70590. lr 2.254819e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 521 iter 22: train loss 0.70801. lr 2.209615e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 522 iter 22: train loss 0.69079. lr 2.164603e-03: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 523 iter 22: train loss 0.68678. lr 2.119793e-03: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 524 iter 22: train loss 0.68940. lr 2.075198e-03: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 525 iter 22: train loss 0.69804. lr 2.030827e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 526 iter 22: train loss 0.68958. lr 1.986692e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 527 iter 22: train loss 0.69630. lr 1.942803e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 528 iter 22: train loss 0.66727. lr 1.899171e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 529 iter 22: train loss 0.67804. lr 1.855807e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 530 iter 22: train loss 0.67496. lr 1.812720e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 531 iter 22: train loss 0.65097. lr 1.769922e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 532 iter 22: train loss 0.65903. lr 1.727422e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 533 iter 22: train loss 0.64017. lr 1.685232e-03: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 534 iter 22: train loss 0.66898. lr 1.643361e-03: 100% 23/23 [00:01<00:00, 18.15it/s]\n","epoch 535 iter 22: train loss 0.66895. lr 1.601820e-03: 100% 23/23 [00:01<00:00, 17.95it/s]\n","epoch 536 iter 22: train loss 0.65735. lr 1.560619e-03: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 537 iter 22: train loss 0.66606. lr 1.519767e-03: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 538 iter 22: train loss 0.65179. lr 1.479275e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 539 iter 22: train loss 0.64519. lr 1.439153e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 540 iter 22: train loss 0.64406. lr 1.399409e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 541 iter 22: train loss 0.64077. lr 1.360055e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 542 iter 22: train loss 0.63880. lr 1.321099e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 543 iter 22: train loss 0.63052. lr 1.282551e-03: 100% 23/23 [00:01<00:00, 17.97it/s]\n","epoch 544 iter 22: train loss 0.64173. lr 1.244420e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 545 iter 22: train loss 0.62727. lr 1.206716e-03: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 546 iter 22: train loss 0.64146. lr 1.169447e-03: 100% 23/23 [00:01<00:00, 18.69it/s]\n","epoch 547 iter 22: train loss 0.64042. lr 1.132623e-03: 100% 23/23 [00:01<00:00, 18.69it/s]\n","epoch 548 iter 22: train loss 0.64791. lr 1.096253e-03: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 549 iter 22: train loss 0.61797. lr 1.060345e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 550 iter 22: train loss 0.59266. lr 1.024909e-03: 100% 23/23 [00:01<00:00, 18.72it/s]\n","epoch 551 iter 22: train loss 0.63583. lr 9.899527e-04: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 552 iter 22: train loss 0.64195. lr 9.554845e-04: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 553 iter 22: train loss 0.61300. lr 9.215132e-04: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 554 iter 22: train loss 0.58359. lr 8.880468e-04: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 555 iter 22: train loss 0.59862. lr 8.550935e-04: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 556 iter 22: train loss 0.58652. lr 8.226614e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 557 iter 22: train loss 0.61082. lr 7.907583e-04: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 558 iter 22: train loss 0.61299. lr 7.593919e-04: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 559 iter 22: train loss 0.59445. lr 7.285699e-04: 100% 23/23 [00:01<00:00, 18.57it/s]\n","epoch 560 iter 22: train loss 0.58705. lr 6.982998e-04: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 561 iter 22: train loss 0.56754. lr 6.685889e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 562 iter 22: train loss 0.60044. lr 6.394445e-04: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 563 iter 22: train loss 0.62136. lr 6.108735e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 564 iter 22: train loss 0.63193. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 565 iter 22: train loss 0.57814. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.99it/s]\n","epoch 566 iter 22: train loss 0.61105. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.62it/s]\n","epoch 567 iter 22: train loss 0.60365. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 568 iter 22: train loss 0.58986. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.25it/s]\n","epoch 569 iter 22: train loss 0.56484. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 570 iter 22: train loss 0.59496. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 571 iter 22: train loss 0.58943. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 572 iter 22: train loss 0.61103. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 573 iter 22: train loss 0.59415. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 574 iter 22: train loss 0.59732. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.07it/s]\n","epoch 575 iter 22: train loss 0.57277. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 576 iter 22: train loss 0.59798. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 577 iter 22: train loss 0.58581. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.50it/s]\n","epoch 578 iter 22: train loss 0.54380. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.63it/s]\n","epoch 579 iter 22: train loss 0.58102. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 580 iter 22: train loss 0.56553. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 581 iter 22: train loss 0.58289. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 582 iter 22: train loss 0.56118. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 583 iter 22: train loss 0.57497. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 584 iter 22: train loss 0.57237. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.82it/s]\n","epoch 585 iter 22: train loss 0.59063. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 586 iter 22: train loss 0.56534. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 587 iter 22: train loss 0.59829. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 588 iter 22: train loss 0.56660. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 589 iter 22: train loss 0.58780. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 590 iter 22: train loss 0.56244. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 591 iter 22: train loss 0.57597. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 592 iter 22: train loss 0.56775. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.21it/s]\n","epoch 593 iter 22: train loss 0.56153. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 594 iter 22: train loss 0.61283. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 595 iter 22: train loss 0.61123. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 596 iter 22: train loss 0.56990. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 597 iter 22: train loss 0.58418. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 598 iter 22: train loss 0.56263. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.18it/s]\n","epoch 599 iter 22: train loss 0.59464. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.23it/s]\n","epoch 600 iter 22: train loss 0.58518. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 601 iter 22: train loss 0.61536. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 602 iter 22: train loss 0.57805. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 603 iter 22: train loss 0.58075. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 604 iter 22: train loss 0.58934. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.15it/s]\n","epoch 605 iter 22: train loss 0.56773. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 606 iter 22: train loss 0.57118. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 607 iter 22: train loss 0.56779. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 608 iter 22: train loss 0.58680. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.42it/s]\n","epoch 609 iter 22: train loss 0.56921. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 610 iter 22: train loss 0.59562. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 611 iter 22: train loss 0.59124. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 612 iter 22: train loss 0.56774. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 613 iter 22: train loss 0.55989. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 614 iter 22: train loss 0.59791. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 615 iter 22: train loss 0.59922. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 616 iter 22: train loss 0.57330. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.69it/s]\n","epoch 617 iter 22: train loss 0.57084. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 618 iter 22: train loss 0.57264. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.19it/s]\n","epoch 619 iter 22: train loss 0.57472. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.11it/s]\n","epoch 620 iter 22: train loss 0.56892. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 621 iter 22: train loss 0.57146. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 622 iter 22: train loss 0.57728. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 623 iter 22: train loss 0.57519. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 624 iter 22: train loss 0.57546. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 625 iter 22: train loss 0.55731. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.30it/s]\n","epoch 626 iter 22: train loss 0.55790. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 627 iter 22: train loss 0.57069. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 628 iter 22: train loss 0.58436. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 629 iter 22: train loss 0.58513. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 630 iter 22: train loss 0.58770. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 631 iter 22: train loss 0.58251. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.31it/s]\n","epoch 632 iter 22: train loss 0.56472. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 633 iter 22: train loss 0.60995. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.25it/s]\n","epoch 634 iter 22: train loss 0.58167. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.69it/s]\n","epoch 635 iter 22: train loss 0.57482. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 636 iter 22: train loss 0.57369. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 637 iter 22: train loss 0.58968. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 638 iter 22: train loss 0.55513. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 639 iter 22: train loss 0.56257. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 640 iter 22: train loss 0.56522. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 641 iter 22: train loss 0.55069. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.01it/s]\n","epoch 642 iter 22: train loss 0.54138. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 643 iter 22: train loss 0.56783. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 644 iter 22: train loss 0.55133. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 645 iter 22: train loss 0.56184. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 646 iter 22: train loss 0.57593. lr 6.013192e-04: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 647 iter 22: train loss 0.60181. lr 6.296941e-04: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 648 iter 22: train loss 0.56966. lr 6.586449e-04: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 649 iter 22: train loss 0.56510. lr 6.881646e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 650 iter 22: train loss 0.59331. lr 7.182460e-04: 100% 23/23 [00:01<00:00, 18.46it/s]\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_finetune_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeQHu9PkysX1","executionInfo":{"status":"ok","timestamp":1732597216152,"user_tz":480,"elapsed":14776,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"5e2371a0-d311-461b-d804-8bf5936efbef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:00:03.132311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:00:03.150977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:00:03.171798: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:00:03.178100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:00:03.193188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:00:04.337622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","epoch 1 iter 7: train loss 0.78924. lr 5.999844e-04: 100% 8/8 [00:01<00:00,  7.59it/s]\n","epoch 2 iter 7: train loss 0.61906. lr 5.999351e-04: 100% 8/8 [00:00<00:00, 10.79it/s]\n","epoch 3 iter 7: train loss 0.54007. lr 5.998520e-04: 100% 8/8 [00:00<00:00, 10.81it/s]\n","epoch 4 iter 7: train loss 0.47198. lr 5.997351e-04: 100% 8/8 [00:00<00:00, 10.61it/s]\n","epoch 5 iter 7: train loss 0.43504. lr 5.995844e-04: 100% 8/8 [00:00<00:00, 10.63it/s]\n","epoch 6 iter 7: train loss 0.37501. lr 5.993999e-04: 100% 8/8 [00:00<00:00, 10.66it/s]\n","epoch 7 iter 7: train loss 0.33758. lr 5.991818e-04: 100% 8/8 [00:00<00:00, 10.63it/s]\n","epoch 8 iter 7: train loss 0.27114. lr 5.989299e-04: 100% 8/8 [00:00<00:00, 10.73it/s]\n","epoch 9 iter 7: train loss 0.23319. lr 5.986444e-04: 100% 8/8 [00:00<00:00, 10.64it/s]\n","epoch 10 iter 7: train loss 0.22054. lr 5.983252e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_eval_dev_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ui1Py_qSy5Ar","executionInfo":{"status":"ok","timestamp":1732597283457,"user_tz":480,"elapsed":67308,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"72f714ea-16ac-4afa-a36d-341cce0b812f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:00:18.164694: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:00:18.183238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:00:18.204802: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:00:18.211219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:00:18.226470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:00:19.408736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:00,  8.21it/s]\n","Correct: 106.0 out of 500.0: 21.2%\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh vanilla_eval_test_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-iLkL6Py5GZ","executionInfo":{"status":"ok","timestamp":1732597342412,"user_tz":480,"elapsed":58958,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"e7a3f648-1ad4-42be-eaea-d61fffcf23db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:01:25.343525: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:01:25.362508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:01:25.383847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:01:25.390537: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:01:25.405800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:01:26.603020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:52,  8.33it/s]\n","!!! No ground truth is provided, this will be done on the autograder, returning (0,0)\n","Predictions written to ./submission/vanilla.pretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"markdown","source":["# Q(g)"],"metadata":{"id":"6IAs0yRp4svg"}},{"cell_type":"code","source":["!cd src && bash run.sh perceiver_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nl4CIHyz4sG7","executionInfo":{"status":"ok","timestamp":1732598628692,"user_tz":480,"elapsed":478212,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"a38f847f-dfb1-4212-aebc-c033e63959c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Perceiver Pretrain: ~ 2 Hours\n","2024-11-26 05:15:52.533544: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:15:52.552892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:15:52.575045: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:15:52.581843: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:15:52.597486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:15:53.812213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","epoch 1 iter 22: train loss 3.45510. lr 5.999655e-03: 100% 23/23 [00:01<00:00, 21.56it/s]\n","epoch 2 iter 22: train loss 3.22319. lr 5.998582e-03: 100% 23/23 [00:00<00:00, 29.99it/s]\n","epoch 3 iter 22: train loss 3.07360. lr 5.996780e-03: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 4 iter 22: train loss 2.94501. lr 5.994250e-03: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 5 iter 22: train loss 2.89056. lr 5.990993e-03: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 6 iter 22: train loss 2.83674. lr 5.987009e-03: 100% 23/23 [00:00<00:00, 31.44it/s]\n","epoch 7 iter 22: train loss 2.79955. lr 5.982299e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 8 iter 22: train loss 2.75071. lr 5.976865e-03: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 9 iter 22: train loss 2.74061. lr 5.970707e-03: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 10 iter 22: train loss 2.71405. lr 5.963828e-03: 100% 23/23 [00:00<00:00, 31.37it/s]\n","epoch 11 iter 22: train loss 2.68511. lr 5.956228e-03: 100% 23/23 [00:00<00:00, 31.23it/s]\n","epoch 12 iter 22: train loss 2.68936. lr 5.947911e-03: 100% 23/23 [00:00<00:00, 30.83it/s]\n","epoch 13 iter 22: train loss 2.67541. lr 5.938877e-03: 100% 23/23 [00:00<00:00, 30.64it/s]\n","epoch 14 iter 22: train loss 2.63965. lr 5.929129e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 15 iter 22: train loss 2.64489. lr 5.918669e-03: 100% 23/23 [00:00<00:00, 31.70it/s]\n","epoch 16 iter 22: train loss 2.63455. lr 5.907500e-03: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 17 iter 22: train loss 2.62923. lr 5.895625e-03: 100% 23/23 [00:00<00:00, 31.52it/s]\n","epoch 18 iter 22: train loss 2.62699. lr 5.883046e-03: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 19 iter 22: train loss 2.60317. lr 5.869767e-03: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 20 iter 22: train loss 2.60567. lr 5.855791e-03: 100% 23/23 [00:00<00:00, 27.54it/s]\n","epoch 21 iter 22: train loss 2.61343. lr 5.841120e-03: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 22 iter 22: train loss 2.58649. lr 5.825760e-03: 100% 23/23 [00:00<00:00, 32.90it/s]\n","epoch 23 iter 22: train loss 2.58636. lr 5.809713e-03: 100% 23/23 [00:00<00:00, 33.05it/s]\n","epoch 24 iter 22: train loss 2.56966. lr 5.792983e-03: 100% 23/23 [00:00<00:00, 33.22it/s]\n","epoch 25 iter 22: train loss 2.57560. lr 5.775575e-03: 100% 23/23 [00:00<00:00, 32.75it/s]\n","epoch 26 iter 22: train loss 2.57401. lr 5.757492e-03: 100% 23/23 [00:00<00:00, 33.05it/s]\n","epoch 27 iter 22: train loss 2.59402. lr 5.738739e-03: 100% 23/23 [00:00<00:00, 32.84it/s]\n","epoch 28 iter 22: train loss 2.58086. lr 5.719321e-03: 100% 23/23 [00:00<00:00, 32.68it/s]\n","epoch 29 iter 22: train loss 2.56697. lr 5.699242e-03: 100% 23/23 [00:00<00:00, 31.64it/s]\n","epoch 30 iter 22: train loss 2.56294. lr 5.678508e-03: 100% 23/23 [00:00<00:00, 29.15it/s]\n","epoch 31 iter 22: train loss 2.56733. lr 5.657123e-03: 100% 23/23 [00:00<00:00, 32.70it/s]\n","epoch 32 iter 22: train loss 2.54572. lr 5.635092e-03: 100% 23/23 [00:00<00:00, 32.88it/s]\n","epoch 33 iter 22: train loss 2.55551. lr 5.612421e-03: 100% 23/23 [00:00<00:00, 31.25it/s]\n","epoch 34 iter 22: train loss 2.54028. lr 5.589115e-03: 100% 23/23 [00:00<00:00, 31.16it/s]\n","epoch 35 iter 22: train loss 2.54270. lr 5.565180e-03: 100% 23/23 [00:00<00:00, 31.21it/s]\n","epoch 36 iter 22: train loss 2.53582. lr 5.540622e-03: 100% 23/23 [00:00<00:00, 31.17it/s]\n","epoch 37 iter 22: train loss 2.53624. lr 5.515446e-03: 100% 23/23 [00:00<00:00, 31.37it/s]\n","epoch 38 iter 22: train loss 2.53060. lr 5.489660e-03: 100% 23/23 [00:00<00:00, 32.42it/s]\n","epoch 39 iter 22: train loss 2.53203. lr 5.463268e-03: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 40 iter 22: train loss 2.51772. lr 5.436278e-03: 100% 23/23 [00:00<00:00, 32.08it/s]\n","epoch 41 iter 22: train loss 2.50836. lr 5.408696e-03: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 42 iter 22: train loss 2.50127. lr 5.380529e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 43 iter 22: train loss 2.49243. lr 5.351784e-03: 100% 23/23 [00:00<00:00, 30.96it/s]\n","epoch 44 iter 22: train loss 2.48617. lr 5.322467e-03: 100% 23/23 [00:00<00:00, 28.83it/s]\n","epoch 45 iter 22: train loss 2.51583. lr 5.292586e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 46 iter 22: train loss 2.49644. lr 5.262148e-03: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 47 iter 22: train loss 2.46038. lr 5.231160e-03: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 48 iter 22: train loss 2.48909. lr 5.199630e-03: 100% 23/23 [00:00<00:00, 32.16it/s]\n","epoch 49 iter 22: train loss 2.46154. lr 5.167566e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 50 iter 22: train loss 2.47309. lr 5.134975e-03: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 51 iter 22: train loss 2.48672. lr 5.101865e-03: 100% 23/23 [00:00<00:00, 32.30it/s]\n","epoch 52 iter 22: train loss 2.47398. lr 5.068245e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 53 iter 22: train loss 2.45579. lr 5.034122e-03: 100% 23/23 [00:00<00:00, 31.82it/s]\n","epoch 54 iter 22: train loss 2.46923. lr 4.999505e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 55 iter 22: train loss 2.46120. lr 4.964402e-03: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 56 iter 22: train loss 2.46662. lr 4.928822e-03: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 57 iter 22: train loss 2.45646. lr 4.892774e-03: 100% 23/23 [00:00<00:00, 31.37it/s]\n","epoch 58 iter 22: train loss 2.45487. lr 4.856265e-03: 100% 23/23 [00:00<00:00, 32.19it/s]\n","epoch 59 iter 22: train loss 2.45852. lr 4.819305e-03: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 60 iter 22: train loss 2.43254. lr 4.781904e-03: 100% 23/23 [00:00<00:00, 32.23it/s]\n","epoch 61 iter 22: train loss 2.42239. lr 4.744069e-03: 100% 23/23 [00:00<00:00, 32.35it/s]\n","epoch 62 iter 22: train loss 2.43151. lr 4.705811e-03: 100% 23/23 [00:00<00:00, 31.59it/s]\n","epoch 63 iter 22: train loss 2.41923. lr 4.667138e-03: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 64 iter 22: train loss 2.40608. lr 4.628061e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 65 iter 22: train loss 2.38516. lr 4.588587e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 66 iter 22: train loss 2.36483. lr 4.548728e-03: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 67 iter 22: train loss 2.36255. lr 4.508492e-03: 100% 23/23 [00:00<00:00, 32.72it/s]\n","epoch 68 iter 22: train loss 2.38590. lr 4.467890e-03: 100% 23/23 [00:00<00:00, 32.40it/s]\n","epoch 69 iter 22: train loss 2.42016. lr 4.426932e-03: 100% 23/23 [00:00<00:00, 31.84it/s]\n","epoch 70 iter 22: train loss 2.38813. lr 4.385626e-03: 100% 23/23 [00:00<00:00, 32.38it/s]\n","epoch 71 iter 22: train loss 2.38421. lr 4.343984e-03: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 72 iter 22: train loss 2.37121. lr 4.302016e-03: 100% 23/23 [00:00<00:00, 32.58it/s]\n","epoch 73 iter 22: train loss 2.36629. lr 4.259731e-03: 100% 23/23 [00:00<00:00, 32.84it/s]\n","epoch 74 iter 22: train loss 2.35554. lr 4.217140e-03: 100% 23/23 [00:00<00:00, 32.22it/s]\n","epoch 75 iter 22: train loss 2.36217. lr 4.174253e-03: 100% 23/23 [00:00<00:00, 32.07it/s]\n","epoch 76 iter 22: train loss 2.36015. lr 4.131081e-03: 100% 23/23 [00:00<00:00, 32.31it/s]\n","epoch 77 iter 22: train loss 2.34041. lr 4.087634e-03: 100% 23/23 [00:00<00:00, 32.53it/s]\n","epoch 78 iter 22: train loss 2.35500. lr 4.043923e-03: 100% 23/23 [00:00<00:00, 32.39it/s]\n","epoch 79 iter 22: train loss 2.33786. lr 3.999958e-03: 100% 23/23 [00:00<00:00, 32.12it/s]\n","epoch 80 iter 22: train loss 2.32366. lr 3.955751e-03: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 81 iter 22: train loss 2.35387. lr 3.911311e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 82 iter 22: train loss 2.36603. lr 3.866650e-03: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 83 iter 22: train loss 2.32143. lr 3.821778e-03: 100% 23/23 [00:00<00:00, 32.23it/s]\n","epoch 84 iter 22: train loss 2.35590. lr 3.776706e-03: 100% 23/23 [00:00<00:00, 32.52it/s]\n","epoch 85 iter 22: train loss 2.32006. lr 3.731446e-03: 100% 23/23 [00:00<00:00, 32.80it/s]\n","epoch 86 iter 22: train loss 2.29174. lr 3.686008e-03: 100% 23/23 [00:00<00:00, 30.94it/s]\n","epoch 87 iter 22: train loss 2.32077. lr 3.640404e-03: 100% 23/23 [00:00<00:00, 32.64it/s]\n","epoch 88 iter 22: train loss 2.30481. lr 3.594643e-03: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 89 iter 22: train loss 2.29797. lr 3.548739e-03: 100% 23/23 [00:00<00:00, 31.50it/s]\n","epoch 90 iter 22: train loss 2.30791. lr 3.502701e-03: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 91 iter 22: train loss 2.31087. lr 3.456541e-03: 100% 23/23 [00:00<00:00, 32.27it/s]\n","epoch 92 iter 22: train loss 2.30896. lr 3.410270e-03: 100% 23/23 [00:00<00:00, 32.03it/s]\n","epoch 93 iter 22: train loss 2.28962. lr 3.363899e-03: 100% 23/23 [00:00<00:00, 32.23it/s]\n","epoch 94 iter 22: train loss 2.26783. lr 3.317440e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 95 iter 22: train loss 2.25137. lr 3.270903e-03: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 96 iter 22: train loss 2.30658. lr 3.224301e-03: 100% 23/23 [00:00<00:00, 31.36it/s]\n","epoch 97 iter 22: train loss 2.23345. lr 3.177645e-03: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 98 iter 22: train loss 2.24353. lr 3.130945e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 99 iter 22: train loss 2.26341. lr 3.084213e-03: 100% 23/23 [00:00<00:00, 30.39it/s]\n","epoch 100 iter 22: train loss 2.24469. lr 3.037461e-03: 100% 23/23 [00:00<00:00, 29.31it/s]\n","epoch 101 iter 22: train loss 2.26628. lr 2.990700e-03: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 102 iter 22: train loss 2.24059. lr 2.943941e-03: 100% 23/23 [00:00<00:00, 31.69it/s]\n","epoch 103 iter 22: train loss 2.25259. lr 2.897196e-03: 100% 23/23 [00:00<00:00, 31.05it/s]\n","epoch 104 iter 22: train loss 2.20906. lr 2.850476e-03: 100% 23/23 [00:00<00:00, 31.30it/s]\n","epoch 105 iter 22: train loss 2.22661. lr 2.803792e-03: 100% 23/23 [00:00<00:00, 29.97it/s]\n","epoch 106 iter 22: train loss 2.24367. lr 2.757156e-03: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 107 iter 22: train loss 2.24195. lr 2.710579e-03: 100% 23/23 [00:00<00:00, 31.75it/s]\n","epoch 108 iter 22: train loss 2.24547. lr 2.664072e-03: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 109 iter 22: train loss 2.25207. lr 2.617646e-03: 100% 23/23 [00:00<00:00, 32.00it/s]\n","epoch 110 iter 22: train loss 2.20804. lr 2.571314e-03: 100% 23/23 [00:00<00:00, 31.66it/s]\n","epoch 111 iter 22: train loss 2.22386. lr 2.525086e-03: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 112 iter 22: train loss 2.22919. lr 2.478973e-03: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 113 iter 22: train loss 2.23213. lr 2.432986e-03: 100% 23/23 [00:00<00:00, 30.06it/s]\n","epoch 114 iter 22: train loss 2.19779. lr 2.387138e-03: 100% 23/23 [00:00<00:00, 30.89it/s]\n","epoch 115 iter 22: train loss 2.22067. lr 2.341438e-03: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 116 iter 22: train loss 2.18511. lr 2.295899e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 117 iter 22: train loss 2.16140. lr 2.250530e-03: 100% 23/23 [00:00<00:00, 32.26it/s]\n","epoch 118 iter 22: train loss 2.18020. lr 2.205344e-03: 100% 23/23 [00:00<00:00, 32.47it/s]\n","epoch 119 iter 22: train loss 2.21112. lr 2.160350e-03: 100% 23/23 [00:00<00:00, 32.52it/s]\n","epoch 120 iter 22: train loss 2.18036. lr 2.115561e-03: 100% 23/23 [00:00<00:00, 31.96it/s]\n","epoch 121 iter 22: train loss 2.19937. lr 2.070986e-03: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 122 iter 22: train loss 2.17319. lr 2.026638e-03: 100% 23/23 [00:00<00:00, 32.49it/s]\n","epoch 123 iter 22: train loss 2.16949. lr 1.982525e-03: 100% 23/23 [00:00<00:00, 32.45it/s]\n","epoch 124 iter 22: train loss 2.14276. lr 1.938660e-03: 100% 23/23 [00:00<00:00, 32.80it/s]\n","epoch 125 iter 22: train loss 2.19091. lr 1.895053e-03: 100% 23/23 [00:00<00:00, 32.76it/s]\n","epoch 126 iter 22: train loss 2.16674. lr 1.851714e-03: 100% 23/23 [00:00<00:00, 31.31it/s]\n","epoch 127 iter 22: train loss 2.16017. lr 1.808654e-03: 100% 23/23 [00:00<00:00, 30.21it/s]\n","epoch 128 iter 22: train loss 2.16419. lr 1.765884e-03: 100% 23/23 [00:00<00:00, 30.22it/s]\n","epoch 129 iter 22: train loss 2.15300. lr 1.723414e-03: 100% 23/23 [00:00<00:00, 31.02it/s]\n","epoch 130 iter 22: train loss 2.15510. lr 1.681253e-03: 100% 23/23 [00:00<00:00, 30.71it/s]\n","epoch 131 iter 22: train loss 2.15309. lr 1.639413e-03: 100% 23/23 [00:00<00:00, 30.98it/s]\n","epoch 132 iter 22: train loss 2.12968. lr 1.597904e-03: 100% 23/23 [00:00<00:00, 31.24it/s]\n","epoch 133 iter 22: train loss 2.12340. lr 1.556735e-03: 100% 23/23 [00:00<00:00, 29.42it/s]\n","epoch 134 iter 22: train loss 2.15525. lr 1.515917e-03: 100% 23/23 [00:00<00:00, 31.52it/s]\n","epoch 135 iter 22: train loss 2.11060. lr 1.475460e-03: 100% 23/23 [00:00<00:00, 31.97it/s]\n","epoch 136 iter 22: train loss 2.10467. lr 1.435373e-03: 100% 23/23 [00:00<00:00, 31.82it/s]\n","epoch 137 iter 22: train loss 2.09335. lr 1.395666e-03: 100% 23/23 [00:00<00:00, 31.44it/s]\n","epoch 138 iter 22: train loss 2.15214. lr 1.356349e-03: 100% 23/23 [00:00<00:00, 31.69it/s]\n","epoch 139 iter 22: train loss 2.16624. lr 1.317431e-03: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 140 iter 22: train loss 2.17369. lr 1.278922e-03: 100% 23/23 [00:00<00:00, 31.24it/s]\n","epoch 141 iter 22: train loss 2.10481. lr 1.240831e-03: 100% 23/23 [00:00<00:00, 28.70it/s]\n","epoch 142 iter 22: train loss 2.08578. lr 1.203168e-03: 100% 23/23 [00:00<00:00, 31.22it/s]\n","epoch 143 iter 22: train loss 2.12154. lr 1.165941e-03: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 144 iter 22: train loss 2.09403. lr 1.129159e-03: 100% 23/23 [00:00<00:00, 32.26it/s]\n","epoch 145 iter 22: train loss 2.11395. lr 1.092833e-03: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 146 iter 22: train loss 2.11376. lr 1.056969e-03: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 147 iter 22: train loss 2.11545. lr 1.021578e-03: 100% 23/23 [00:00<00:00, 31.62it/s]\n","epoch 148 iter 22: train loss 2.12247. lr 9.866675e-04: 100% 23/23 [00:00<00:00, 31.53it/s]\n","epoch 149 iter 22: train loss 2.12019. lr 9.522460e-04: 100% 23/23 [00:00<00:00, 31.20it/s]\n","epoch 150 iter 22: train loss 2.11745. lr 9.183221e-04: 100% 23/23 [00:00<00:00, 31.39it/s]\n","epoch 151 iter 22: train loss 2.09321. lr 8.849040e-04: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 152 iter 22: train loss 2.10621. lr 8.519997e-04: 100% 23/23 [00:00<00:00, 32.15it/s]\n","epoch 153 iter 22: train loss 2.07648. lr 8.196174e-04: 100% 23/23 [00:00<00:00, 32.43it/s]\n","epoch 154 iter 22: train loss 2.06221. lr 7.877647e-04: 100% 23/23 [00:00<00:00, 31.59it/s]\n","epoch 155 iter 22: train loss 2.10592. lr 7.564496e-04: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 156 iter 22: train loss 2.09082. lr 7.256796e-04: 100% 23/23 [00:00<00:00, 31.31it/s]\n","epoch 157 iter 22: train loss 2.06869. lr 6.954621e-04: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 158 iter 22: train loss 2.08130. lr 6.658045e-04: 100% 23/23 [00:00<00:00, 32.07it/s]\n","epoch 159 iter 22: train loss 2.06992. lr 6.367141e-04: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 160 iter 22: train loss 2.08694. lr 6.081979e-04: 100% 23/23 [00:00<00:00, 31.82it/s]\n","epoch 161 iter 22: train loss 2.04402. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.30it/s]\n","epoch 162 iter 22: train loss 2.06028. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 163 iter 22: train loss 2.05079. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 164 iter 22: train loss 2.05548. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.70it/s]\n","epoch 165 iter 22: train loss 2.07459. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.30it/s]\n","epoch 166 iter 22: train loss 2.02836. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 167 iter 22: train loss 2.05684. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.14it/s]\n","epoch 168 iter 22: train loss 2.06690. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.79it/s]\n","epoch 169 iter 22: train loss 2.02777. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 29.70it/s]\n","epoch 170 iter 22: train loss 2.09978. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.79it/s]\n","epoch 171 iter 22: train loss 2.04383. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.58it/s]\n","epoch 172 iter 22: train loss 2.07170. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 173 iter 22: train loss 2.03699. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 174 iter 22: train loss 2.06653. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 175 iter 22: train loss 2.04384. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 176 iter 22: train loss 2.03521. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.19it/s]\n","epoch 177 iter 22: train loss 2.05879. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 178 iter 22: train loss 2.02941. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 179 iter 22: train loss 2.06717. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 180 iter 22: train loss 2.04491. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 181 iter 22: train loss 2.06749. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 182 iter 22: train loss 2.03651. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.05it/s]\n","epoch 183 iter 22: train loss 2.01657. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.11it/s]\n","epoch 184 iter 22: train loss 2.03699. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.27it/s]\n","epoch 185 iter 22: train loss 2.03253. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 186 iter 22: train loss 2.03013. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.43it/s]\n","epoch 187 iter 22: train loss 1.97188. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.70it/s]\n","epoch 188 iter 22: train loss 2.02869. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.53it/s]\n","epoch 189 iter 22: train loss 2.04381. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 190 iter 22: train loss 2.06350. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 191 iter 22: train loss 2.00380. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.56it/s]\n","epoch 192 iter 22: train loss 1.96229. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.53it/s]\n","epoch 193 iter 22: train loss 2.02016. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.62it/s]\n","epoch 194 iter 22: train loss 1.97396. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.51it/s]\n","epoch 195 iter 22: train loss 2.04154. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.77it/s]\n","epoch 196 iter 22: train loss 2.04241. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.30it/s]\n","epoch 197 iter 22: train loss 2.02029. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.49it/s]\n","epoch 198 iter 22: train loss 2.00816. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.13it/s]\n","epoch 199 iter 22: train loss 2.00066. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.03it/s]\n","epoch 200 iter 22: train loss 2.03211. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.88it/s]\n","epoch 201 iter 22: train loss 2.01407. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.16it/s]\n","epoch 202 iter 22: train loss 2.00730. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.59it/s]\n","epoch 203 iter 22: train loss 2.01560. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 204 iter 22: train loss 2.02799. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.12it/s]\n","epoch 205 iter 22: train loss 1.98761. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 206 iter 22: train loss 2.01125. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.78it/s]\n","epoch 207 iter 22: train loss 1.98445. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 208 iter 22: train loss 2.05330. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 209 iter 22: train loss 2.02308. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.21it/s]\n","epoch 210 iter 22: train loss 2.02930. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 211 iter 22: train loss 1.97619. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.02it/s]\n","epoch 212 iter 22: train loss 2.03329. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.33it/s]\n","epoch 213 iter 22: train loss 1.97187. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 214 iter 22: train loss 2.03611. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 215 iter 22: train loss 1.98446. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.30it/s]\n","epoch 216 iter 22: train loss 2.02069. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.27it/s]\n","epoch 217 iter 22: train loss 2.01460. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.20it/s]\n","epoch 218 iter 22: train loss 2.06917. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.12it/s]\n","epoch 219 iter 22: train loss 1.99676. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.31it/s]\n","epoch 220 iter 22: train loss 2.00218. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.38it/s]\n","epoch 221 iter 22: train loss 1.99128. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 222 iter 22: train loss 1.98277. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.25it/s]\n","epoch 223 iter 22: train loss 1.99853. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.24it/s]\n","epoch 224 iter 22: train loss 2.01223. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.80it/s]\n","epoch 225 iter 22: train loss 2.01299. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 29.96it/s]\n","epoch 226 iter 22: train loss 1.97634. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 227 iter 22: train loss 1.95367. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 228 iter 22: train loss 2.05872. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 229 iter 22: train loss 1.98604. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 230 iter 22: train loss 1.95687. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.20it/s]\n","epoch 231 iter 22: train loss 1.95009. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.42it/s]\n","epoch 232 iter 22: train loss 1.96199. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.97it/s]\n","epoch 233 iter 22: train loss 1.92461. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.26it/s]\n","epoch 234 iter 22: train loss 1.96948. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.25it/s]\n","epoch 235 iter 22: train loss 1.96480. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 236 iter 22: train loss 1.97105. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.23it/s]\n","epoch 237 iter 22: train loss 2.00309. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 238 iter 22: train loss 1.94061. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.00it/s]\n","epoch 239 iter 22: train loss 1.99666. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.70it/s]\n","epoch 240 iter 22: train loss 1.97851. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 241 iter 22: train loss 1.97921. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 242 iter 22: train loss 1.96573. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 243 iter 22: train loss 1.99265. lr 6.039815e-04: 100% 23/23 [00:00<00:00, 31.91it/s]\n","epoch 244 iter 22: train loss 1.96660. lr 6.324112e-04: 100% 23/23 [00:00<00:00, 32.35it/s]\n","epoch 245 iter 22: train loss 1.93466. lr 6.614162e-04: 100% 23/23 [00:00<00:00, 32.55it/s]\n","epoch 246 iter 22: train loss 1.99192. lr 6.909893e-04: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 247 iter 22: train loss 1.97827. lr 7.211235e-04: 100% 23/23 [00:00<00:00, 32.24it/s]\n","epoch 248 iter 22: train loss 1.98331. lr 7.518114e-04: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 249 iter 22: train loss 1.98212. lr 7.830454e-04: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 250 iter 22: train loss 1.95253. lr 8.148181e-04: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 251 iter 22: train loss 1.96363. lr 8.471217e-04: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 252 iter 22: train loss 1.93086. lr 8.799484e-04: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 253 iter 22: train loss 1.98361. lr 9.132902e-04: 100% 23/23 [00:00<00:00, 31.24it/s]\n","epoch 254 iter 22: train loss 1.95788. lr 9.471389e-04: 100% 23/23 [00:00<00:00, 31.49it/s]\n","epoch 255 iter 22: train loss 1.95694. lr 9.814865e-04: 100% 23/23 [00:00<00:00, 31.50it/s]\n","epoch 256 iter 22: train loss 1.94405. lr 1.016324e-03: 100% 23/23 [00:00<00:00, 31.44it/s]\n","epoch 257 iter 22: train loss 1.94423. lr 1.051644e-03: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 258 iter 22: train loss 1.94696. lr 1.087438e-03: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 259 iter 22: train loss 1.94790. lr 1.123696e-03: 100% 23/23 [00:00<00:00, 32.30it/s]\n","epoch 260 iter 22: train loss 1.93642. lr 1.160409e-03: 100% 23/23 [00:00<00:00, 32.44it/s]\n","epoch 261 iter 22: train loss 1.98330. lr 1.197570e-03: 100% 23/23 [00:00<00:00, 32.67it/s]\n","epoch 262 iter 22: train loss 1.93656. lr 1.235169e-03: 100% 23/23 [00:00<00:00, 32.52it/s]\n","epoch 263 iter 22: train loss 1.97566. lr 1.273196e-03: 100% 23/23 [00:00<00:00, 32.90it/s]\n","epoch 264 iter 22: train loss 1.96747. lr 1.311643e-03: 100% 23/23 [00:00<00:00, 32.54it/s]\n","epoch 265 iter 22: train loss 1.93700. lr 1.350501e-03: 100% 23/23 [00:00<00:00, 32.62it/s]\n","epoch 266 iter 22: train loss 1.93457. lr 1.389759e-03: 100% 23/23 [00:00<00:00, 32.83it/s]\n","epoch 267 iter 22: train loss 1.93200. lr 1.429408e-03: 100% 23/23 [00:00<00:00, 32.86it/s]\n","epoch 268 iter 22: train loss 1.93927. lr 1.469439e-03: 100% 23/23 [00:00<00:00, 32.40it/s]\n","epoch 269 iter 22: train loss 1.94811. lr 1.509841e-03: 100% 23/23 [00:00<00:00, 30.82it/s]\n","epoch 270 iter 22: train loss 1.93007. lr 1.550606e-03: 100% 23/23 [00:00<00:00, 30.75it/s]\n","epoch 271 iter 22: train loss 1.94325. lr 1.591723e-03: 100% 23/23 [00:00<00:00, 30.65it/s]\n","epoch 272 iter 22: train loss 1.93926. lr 1.633182e-03: 100% 23/23 [00:00<00:00, 30.90it/s]\n","epoch 273 iter 22: train loss 1.95852. lr 1.674973e-03: 100% 23/23 [00:00<00:00, 30.18it/s]\n","epoch 274 iter 22: train loss 1.94131. lr 1.717086e-03: 100% 23/23 [00:00<00:00, 29.97it/s]\n","epoch 275 iter 22: train loss 1.98076. lr 1.759511e-03: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 276 iter 22: train loss 1.93831. lr 1.802237e-03: 100% 23/23 [00:00<00:00, 31.12it/s]\n","epoch 277 iter 22: train loss 1.94028. lr 1.845254e-03: 100% 23/23 [00:00<00:00, 31.46it/s]\n","epoch 278 iter 22: train loss 1.96465. lr 1.888552e-03: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 279 iter 22: train loss 1.96257. lr 1.932120e-03: 100% 23/23 [00:00<00:00, 31.46it/s]\n","epoch 280 iter 22: train loss 1.95683. lr 1.975947e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 281 iter 22: train loss 1.96477. lr 2.020023e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 282 iter 22: train loss 1.96458. lr 2.064337e-03: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 283 iter 22: train loss 1.93928. lr 2.108878e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 284 iter 22: train loss 1.92119. lr 2.153636e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 285 iter 22: train loss 1.94597. lr 2.198600e-03: 100% 23/23 [00:00<00:00, 31.21it/s]\n","epoch 286 iter 22: train loss 1.95255. lr 2.243758e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 287 iter 22: train loss 1.92442. lr 2.289100e-03: 100% 23/23 [00:00<00:00, 31.42it/s]\n","epoch 288 iter 22: train loss 1.94736. lr 2.334615e-03: 100% 23/23 [00:00<00:00, 31.33it/s]\n","epoch 289 iter 22: train loss 1.93078. lr 2.380291e-03: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 290 iter 22: train loss 1.93641. lr 2.426118e-03: 100% 23/23 [00:00<00:00, 31.55it/s]\n","epoch 291 iter 22: train loss 1.95166. lr 2.472085e-03: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 292 iter 22: train loss 1.93642. lr 2.518179e-03: 100% 23/23 [00:00<00:00, 31.09it/s]\n","epoch 293 iter 22: train loss 1.94327. lr 2.564391e-03: 100% 23/23 [00:00<00:00, 31.28it/s]\n","epoch 294 iter 22: train loss 1.95463. lr 2.610708e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 295 iter 22: train loss 1.88862. lr 2.657121e-03: 100% 23/23 [00:00<00:00, 32.07it/s]\n","epoch 296 iter 22: train loss 1.88528. lr 2.703616e-03: 100% 23/23 [00:00<00:00, 32.15it/s]\n","epoch 297 iter 22: train loss 1.92645. lr 2.750184e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 298 iter 22: train loss 1.89952. lr 2.796812e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 299 iter 22: train loss 1.93254. lr 2.843489e-03: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 300 iter 22: train loss 1.91629. lr 2.890205e-03: 100% 23/23 [00:00<00:00, 31.96it/s]\n","epoch 301 iter 22: train loss 1.94009. lr 2.936947e-03: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 302 iter 22: train loss 1.97995. lr 2.983705e-03: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 303 iter 22: train loss 1.89294. lr 3.030466e-03: 100% 23/23 [00:00<00:00, 31.52it/s]\n","epoch 304 iter 22: train loss 1.91568. lr 3.077220e-03: 100% 23/23 [00:00<00:00, 31.22it/s]\n","epoch 305 iter 22: train loss 1.93602. lr 3.123955e-03: 100% 23/23 [00:00<00:00, 30.58it/s]\n","epoch 306 iter 22: train loss 1.92156. lr 3.170661e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 307 iter 22: train loss 1.89301. lr 3.217324e-03: 100% 23/23 [00:00<00:00, 31.97it/s]\n","epoch 308 iter 22: train loss 1.97668. lr 3.263935e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 309 iter 22: train loss 1.91442. lr 3.310482e-03: 100% 23/23 [00:00<00:00, 31.82it/s]\n","epoch 310 iter 22: train loss 1.89338. lr 3.356954e-03: 100% 23/23 [00:00<00:00, 31.70it/s]\n","epoch 311 iter 22: train loss 1.88554. lr 3.403338e-03: 100% 23/23 [00:00<00:00, 31.71it/s]\n","epoch 312 iter 22: train loss 1.90236. lr 3.449625e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 313 iter 22: train loss 1.90133. lr 3.495802e-03: 100% 23/23 [00:00<00:00, 32.06it/s]\n","epoch 314 iter 22: train loss 1.91052. lr 3.541859e-03: 100% 23/23 [00:00<00:00, 31.48it/s]\n","epoch 315 iter 22: train loss 1.91121. lr 3.587785e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 316 iter 22: train loss 1.88731. lr 3.633567e-03: 100% 23/23 [00:00<00:00, 31.84it/s]\n","epoch 317 iter 22: train loss 1.91431. lr 3.679196e-03: 100% 23/23 [00:00<00:00, 31.39it/s]\n","epoch 318 iter 22: train loss 1.89529. lr 3.724659e-03: 100% 23/23 [00:00<00:00, 31.91it/s]\n","epoch 319 iter 22: train loss 1.88455. lr 3.769947e-03: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 320 iter 22: train loss 1.88325. lr 3.815047e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 321 iter 22: train loss 1.89619. lr 3.859950e-03: 100% 23/23 [00:00<00:00, 31.56it/s]\n","epoch 322 iter 22: train loss 1.90286. lr 3.904643e-03: 100% 23/23 [00:00<00:00, 30.45it/s]\n","epoch 323 iter 22: train loss 1.90865. lr 3.949117e-03: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 324 iter 22: train loss 1.87102. lr 3.993360e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 325 iter 22: train loss 1.88427. lr 4.037362e-03: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 326 iter 22: train loss 1.89897. lr 4.081111e-03: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 327 iter 22: train loss 1.83411. lr 4.124598e-03: 100% 23/23 [00:00<00:00, 30.97it/s]\n","epoch 328 iter 22: train loss 1.92048. lr 4.167812e-03: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 329 iter 22: train loss 1.94556. lr 4.210742e-03: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 330 iter 22: train loss 1.90154. lr 4.253378e-03: 100% 23/23 [00:00<00:00, 32.62it/s]\n","epoch 331 iter 22: train loss 1.89382. lr 4.295709e-03: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 332 iter 22: train loss 1.88243. lr 4.337726e-03: 100% 23/23 [00:00<00:00, 32.65it/s]\n","epoch 333 iter 22: train loss 1.88247. lr 4.379418e-03: 100% 23/23 [00:00<00:00, 32.48it/s]\n","epoch 334 iter 22: train loss 1.87961. lr 4.420774e-03: 100% 23/23 [00:00<00:00, 32.50it/s]\n","epoch 335 iter 22: train loss 1.87354. lr 4.461785e-03: 100% 23/23 [00:00<00:00, 32.85it/s]\n","epoch 336 iter 22: train loss 1.89125. lr 4.502441e-03: 100% 23/23 [00:00<00:00, 32.35it/s]\n","epoch 337 iter 22: train loss 1.89030. lr 4.542732e-03: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 338 iter 22: train loss 1.86797. lr 4.582648e-03: 100% 23/23 [00:00<00:00, 32.33it/s]\n","epoch 339 iter 22: train loss 1.85326. lr 4.622180e-03: 100% 23/23 [00:00<00:00, 32.42it/s]\n","epoch 340 iter 22: train loss 1.90074. lr 4.661318e-03: 100% 23/23 [00:00<00:00, 32.79it/s]\n","epoch 341 iter 22: train loss 1.81416. lr 4.700052e-03: 100% 23/23 [00:00<00:00, 31.09it/s]\n","epoch 342 iter 22: train loss 1.91041. lr 4.738372e-03: 100% 23/23 [00:00<00:00, 31.05it/s]\n","epoch 343 iter 22: train loss 1.85829. lr 4.776271e-03: 100% 23/23 [00:00<00:00, 31.11it/s]\n","epoch 344 iter 22: train loss 1.90755. lr 4.813738e-03: 100% 23/23 [00:00<00:00, 31.06it/s]\n","epoch 345 iter 22: train loss 1.84595. lr 4.850764e-03: 100% 23/23 [00:00<00:00, 31.25it/s]\n","epoch 346 iter 22: train loss 1.81518. lr 4.887341e-03: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 347 iter 22: train loss 1.86229. lr 4.923459e-03: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 348 iter 22: train loss 1.86036. lr 4.959110e-03: 100% 23/23 [00:00<00:00, 31.63it/s]\n","epoch 349 iter 22: train loss 1.81913. lr 4.994284e-03: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 350 iter 22: train loss 1.86546. lr 5.028975e-03: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 351 iter 22: train loss 1.78282. lr 5.063172e-03: 100% 23/23 [00:00<00:00, 31.63it/s]\n","epoch 352 iter 22: train loss 1.86842. lr 5.096868e-03: 100% 23/23 [00:00<00:00, 31.64it/s]\n","epoch 353 iter 22: train loss 1.82595. lr 5.130054e-03: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 354 iter 22: train loss 1.88028. lr 5.162723e-03: 100% 23/23 [00:00<00:00, 31.45it/s]\n","epoch 355 iter 22: train loss 1.87805. lr 5.194867e-03: 100% 23/23 [00:00<00:00, 31.26it/s]\n","epoch 356 iter 22: train loss 1.83474. lr 5.226477e-03: 100% 23/23 [00:00<00:00, 31.06it/s]\n","epoch 357 iter 22: train loss 1.81292. lr 5.257546e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 358 iter 22: train loss 1.79754. lr 5.288067e-03: 100% 23/23 [00:00<00:00, 32.03it/s]\n","epoch 359 iter 22: train loss 1.84288. lr 5.318032e-03: 100% 23/23 [00:00<00:00, 31.30it/s]\n","epoch 360 iter 22: train loss 1.82156. lr 5.347434e-03: 100% 23/23 [00:00<00:00, 30.99it/s]\n","epoch 361 iter 22: train loss 1.81061. lr 5.376265e-03: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 362 iter 22: train loss 1.79037. lr 5.404519e-03: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 363 iter 22: train loss 1.79962. lr 5.432189e-03: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 364 iter 22: train loss 1.81582. lr 5.459268e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 365 iter 22: train loss 1.80812. lr 5.485750e-03: 100% 23/23 [00:00<00:00, 31.79it/s]\n","epoch 366 iter 22: train loss 1.81142. lr 5.511627e-03: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 367 iter 22: train loss 1.83713. lr 5.536894e-03: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 368 iter 22: train loss 1.83651. lr 5.561545e-03: 100% 23/23 [00:00<00:00, 31.91it/s]\n","epoch 369 iter 22: train loss 1.77982. lr 5.585574e-03: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 370 iter 22: train loss 1.79943. lr 5.608974e-03: 100% 23/23 [00:00<00:00, 31.67it/s]\n","epoch 371 iter 22: train loss 1.78970. lr 5.631741e-03: 100% 23/23 [00:00<00:00, 31.59it/s]\n","epoch 372 iter 22: train loss 1.81705. lr 5.653868e-03: 100% 23/23 [00:00<00:00, 31.65it/s]\n","epoch 373 iter 22: train loss 1.79694. lr 5.675350e-03: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 374 iter 22: train loss 1.80266. lr 5.696182e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 375 iter 22: train loss 1.80448. lr 5.716359e-03: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 376 iter 22: train loss 1.83401. lr 5.735876e-03: 100% 23/23 [00:00<00:00, 32.52it/s]\n","epoch 377 iter 22: train loss 1.78990. lr 5.754729e-03: 100% 23/23 [00:00<00:00, 31.14it/s]\n","epoch 378 iter 22: train loss 1.78600. lr 5.772912e-03: 100% 23/23 [00:00<00:00, 32.24it/s]\n","epoch 379 iter 22: train loss 1.77111. lr 5.790422e-03: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 380 iter 22: train loss 1.74305. lr 5.807253e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 381 iter 22: train loss 1.80559. lr 5.823403e-03: 100% 23/23 [00:00<00:00, 32.24it/s]\n","epoch 382 iter 22: train loss 1.78695. lr 5.838866e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 383 iter 22: train loss 1.79222. lr 5.853640e-03: 100% 23/23 [00:00<00:00, 31.31it/s]\n","epoch 384 iter 22: train loss 1.77828. lr 5.867720e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 385 iter 22: train loss 1.79457. lr 5.881104e-03: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 386 iter 22: train loss 1.76759. lr 5.893788e-03: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 387 iter 22: train loss 1.76915. lr 5.905768e-03: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 388 iter 22: train loss 1.74128. lr 5.917043e-03: 100% 23/23 [00:00<00:00, 32.27it/s]\n","epoch 389 iter 22: train loss 1.76849. lr 5.927609e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 390 iter 22: train loss 1.74260. lr 5.937464e-03: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 391 iter 22: train loss 1.77034. lr 5.946605e-03: 100% 23/23 [00:00<00:00, 32.32it/s]\n","epoch 392 iter 22: train loss 1.73273. lr 5.955030e-03: 100% 23/23 [00:00<00:00, 32.16it/s]\n","epoch 393 iter 22: train loss 1.80614. lr 5.962737e-03: 100% 23/23 [00:00<00:00, 32.29it/s]\n","epoch 394 iter 22: train loss 1.76849. lr 5.969724e-03: 100% 23/23 [00:00<00:00, 32.00it/s]\n","epoch 395 iter 22: train loss 1.79037. lr 5.975990e-03: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 396 iter 22: train loss 1.80209. lr 5.981532e-03: 100% 23/23 [00:00<00:00, 31.55it/s]\n","epoch 397 iter 22: train loss 1.74142. lr 5.986351e-03: 100% 23/23 [00:00<00:00, 32.15it/s]\n","epoch 398 iter 22: train loss 1.72610. lr 5.990443e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 399 iter 22: train loss 1.74671. lr 5.993809e-03: 100% 23/23 [00:00<00:00, 31.75it/s]\n","epoch 400 iter 22: train loss 1.74881. lr 5.996448e-03: 100% 23/23 [00:00<00:00, 32.06it/s]\n","epoch 401 iter 22: train loss 1.74242. lr 5.998359e-03: 100% 23/23 [00:00<00:00, 32.56it/s]\n","epoch 402 iter 22: train loss 1.73239. lr 5.999541e-03: 100% 23/23 [00:00<00:00, 32.95it/s]\n","epoch 403 iter 22: train loss 1.76900. lr 5.999995e-03: 100% 23/23 [00:00<00:00, 32.97it/s]\n","epoch 404 iter 22: train loss 1.78871. lr 5.999719e-03: 100% 23/23 [00:00<00:00, 32.43it/s]\n","epoch 405 iter 22: train loss 1.74112. lr 5.998715e-03: 100% 23/23 [00:00<00:00, 32.86it/s]\n","epoch 406 iter 22: train loss 1.80263. lr 5.996982e-03: 100% 23/23 [00:00<00:00, 32.63it/s]\n","epoch 407 iter 22: train loss 1.67943. lr 5.994521e-03: 100% 23/23 [00:00<00:00, 32.92it/s]\n","epoch 408 iter 22: train loss 1.74476. lr 5.991333e-03: 100% 23/23 [00:00<00:00, 33.08it/s]\n","epoch 409 iter 22: train loss 1.73716. lr 5.987417e-03: 100% 23/23 [00:00<00:00, 32.94it/s]\n","epoch 410 iter 22: train loss 1.69618. lr 5.982776e-03: 100% 23/23 [00:00<00:00, 33.16it/s]\n","epoch 411 iter 22: train loss 1.74359. lr 5.977411e-03: 100% 23/23 [00:00<00:00, 32.50it/s]\n","epoch 412 iter 22: train loss 1.76810. lr 5.971321e-03: 100% 23/23 [00:00<00:00, 32.36it/s]\n","epoch 413 iter 22: train loss 1.70403. lr 5.964510e-03: 100% 23/23 [00:00<00:00, 31.12it/s]\n","epoch 414 iter 22: train loss 1.70793. lr 5.956979e-03: 100% 23/23 [00:00<00:00, 31.10it/s]\n","epoch 415 iter 22: train loss 1.72811. lr 5.948729e-03: 100% 23/23 [00:00<00:00, 30.97it/s]\n","epoch 416 iter 22: train loss 1.73758. lr 5.939763e-03: 100% 23/23 [00:00<00:00, 31.31it/s]\n","epoch 417 iter 22: train loss 1.75116. lr 5.930082e-03: 100% 23/23 [00:00<00:00, 31.56it/s]\n","epoch 418 iter 22: train loss 1.70708. lr 5.919690e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 419 iter 22: train loss 1.75126. lr 5.908588e-03: 100% 23/23 [00:00<00:00, 32.41it/s]\n","epoch 420 iter 22: train loss 1.68270. lr 5.896780e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 421 iter 22: train loss 1.74246. lr 5.884268e-03: 100% 23/23 [00:00<00:00, 32.19it/s]\n","epoch 422 iter 22: train loss 1.74322. lr 5.871055e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 423 iter 22: train loss 1.73892. lr 5.857144e-03: 100% 23/23 [00:00<00:00, 31.82it/s]\n","epoch 424 iter 22: train loss 1.70728. lr 5.842539e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 425 iter 22: train loss 1.64731. lr 5.827244e-03: 100% 23/23 [00:00<00:00, 32.27it/s]\n","epoch 426 iter 22: train loss 1.70579. lr 5.811262e-03: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 427 iter 22: train loss 1.66685. lr 5.794596e-03: 100% 23/23 [00:00<00:00, 31.96it/s]\n","epoch 428 iter 22: train loss 1.71546. lr 5.777252e-03: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 429 iter 22: train loss 1.74034. lr 5.759233e-03: 100% 23/23 [00:00<00:00, 31.57it/s]\n","epoch 430 iter 22: train loss 1.66580. lr 5.740544e-03: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 431 iter 22: train loss 1.66965. lr 5.721189e-03: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 432 iter 22: train loss 1.74095. lr 5.701172e-03: 100% 23/23 [00:00<00:00, 31.42it/s]\n","epoch 433 iter 22: train loss 1.66660. lr 5.680499e-03: 100% 23/23 [00:00<00:00, 32.11it/s]\n","epoch 434 iter 22: train loss 1.71790. lr 5.659176e-03: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 435 iter 22: train loss 1.67146. lr 5.637206e-03: 100% 23/23 [00:00<00:00, 32.39it/s]\n","epoch 436 iter 22: train loss 1.72510. lr 5.614595e-03: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 437 iter 22: train loss 1.67507. lr 5.591349e-03: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 438 iter 22: train loss 1.65672. lr 5.567473e-03: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 439 iter 22: train loss 1.70808. lr 5.542974e-03: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 440 iter 22: train loss 1.65170. lr 5.517857e-03: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 441 iter 22: train loss 1.63440. lr 5.492128e-03: 100% 23/23 [00:00<00:00, 31.52it/s]\n","epoch 442 iter 22: train loss 1.66428. lr 5.465793e-03: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 443 iter 22: train loss 1.60488. lr 5.438860e-03: 100% 23/23 [00:00<00:00, 31.68it/s]\n","epoch 444 iter 22: train loss 1.69706. lr 5.411333e-03: 100% 23/23 [00:00<00:00, 30.52it/s]\n","epoch 445 iter 22: train loss 1.63820. lr 5.383222e-03: 100% 23/23 [00:00<00:00, 31.78it/s]\n","epoch 446 iter 22: train loss 1.62151. lr 5.354531e-03: 100% 23/23 [00:00<00:00, 31.53it/s]\n","epoch 447 iter 22: train loss 1.59607. lr 5.325267e-03: 100% 23/23 [00:00<00:00, 31.41it/s]\n","epoch 448 iter 22: train loss 1.61446. lr 5.295439e-03: 100% 23/23 [00:00<00:00, 31.56it/s]\n","epoch 449 iter 22: train loss 1.63737. lr 5.265054e-03: 100% 23/23 [00:00<00:00, 31.55it/s]\n","epoch 450 iter 22: train loss 1.64729. lr 5.234118e-03: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 451 iter 22: train loss 1.65748. lr 5.202639e-03: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 452 iter 22: train loss 1.65611. lr 5.170625e-03: 100% 23/23 [00:00<00:00, 31.90it/s]\n","epoch 453 iter 22: train loss 1.61540. lr 5.138084e-03: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 454 iter 22: train loss 1.65036. lr 5.105023e-03: 100% 23/23 [00:00<00:00, 32.62it/s]\n","epoch 455 iter 22: train loss 1.63060. lr 5.071450e-03: 100% 23/23 [00:00<00:00, 31.99it/s]\n","epoch 456 iter 22: train loss 1.61005. lr 5.037375e-03: 100% 23/23 [00:00<00:00, 31.65it/s]\n","epoch 457 iter 22: train loss 1.62368. lr 5.002804e-03: 100% 23/23 [00:00<00:00, 31.64it/s]\n","epoch 458 iter 22: train loss 1.62000. lr 4.967747e-03: 100% 23/23 [00:00<00:00, 32.26it/s]\n","epoch 459 iter 22: train loss 1.65711. lr 4.932212e-03: 100% 23/23 [00:00<00:00, 32.73it/s]\n","epoch 460 iter 22: train loss 1.64925. lr 4.896207e-03: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 461 iter 22: train loss 1.57132. lr 4.859742e-03: 100% 23/23 [00:00<00:00, 32.33it/s]\n","epoch 462 iter 22: train loss 1.63434. lr 4.822825e-03: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 463 iter 22: train loss 1.65189. lr 4.785465e-03: 100% 23/23 [00:00<00:00, 31.71it/s]\n","epoch 464 iter 22: train loss 1.62282. lr 4.747671e-03: 100% 23/23 [00:00<00:00, 32.21it/s]\n","epoch 465 iter 22: train loss 1.57237. lr 4.709452e-03: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 466 iter 22: train loss 1.62530. lr 4.670818e-03: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 467 iter 22: train loss 1.62385. lr 4.631778e-03: 100% 23/23 [00:00<00:00, 32.11it/s]\n","epoch 468 iter 22: train loss 1.63135. lr 4.592342e-03: 100% 23/23 [00:00<00:00, 32.33it/s]\n","epoch 469 iter 22: train loss 1.63451. lr 4.552519e-03: 100% 23/23 [00:00<00:00, 32.17it/s]\n","epoch 470 iter 22: train loss 1.60807. lr 4.512319e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 471 iter 22: train loss 1.57703. lr 4.471751e-03: 100% 23/23 [00:00<00:00, 32.46it/s]\n","epoch 472 iter 22: train loss 1.55605. lr 4.430825e-03: 100% 23/23 [00:00<00:00, 31.64it/s]\n","epoch 473 iter 22: train loss 1.58128. lr 4.389552e-03: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 474 iter 22: train loss 1.61459. lr 4.347942e-03: 100% 23/23 [00:00<00:00, 31.51it/s]\n","epoch 475 iter 22: train loss 1.56094. lr 4.306004e-03: 100% 23/23 [00:00<00:00, 31.51it/s]\n","epoch 476 iter 22: train loss 1.54690. lr 4.263748e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 477 iter 22: train loss 1.58138. lr 4.221186e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 478 iter 22: train loss 1.55810. lr 4.178327e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 479 iter 22: train loss 1.58121. lr 4.135181e-03: 100% 23/23 [00:00<00:00, 31.51it/s]\n","epoch 480 iter 22: train loss 1.52641. lr 4.091760e-03: 100% 23/23 [00:00<00:00, 32.04it/s]\n","epoch 481 iter 22: train loss 1.59629. lr 4.048074e-03: 100% 23/23 [00:00<00:00, 31.47it/s]\n","epoch 482 iter 22: train loss 1.54361. lr 4.004132e-03: 100% 23/23 [00:00<00:00, 31.56it/s]\n","epoch 483 iter 22: train loss 1.58826. lr 3.959947e-03: 100% 23/23 [00:00<00:00, 31.23it/s]\n","epoch 484 iter 22: train loss 1.56434. lr 3.915529e-03: 100% 23/23 [00:00<00:00, 31.93it/s]\n","epoch 485 iter 22: train loss 1.52764. lr 3.870888e-03: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 486 iter 22: train loss 1.53845. lr 3.826036e-03: 100% 23/23 [00:00<00:00, 31.84it/s]\n","epoch 487 iter 22: train loss 1.54211. lr 3.780983e-03: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 488 iter 22: train loss 1.55701. lr 3.735740e-03: 100% 23/23 [00:00<00:00, 31.91it/s]\n","epoch 489 iter 22: train loss 1.56716. lr 3.690318e-03: 100% 23/23 [00:00<00:00, 31.44it/s]\n","epoch 490 iter 22: train loss 1.52396. lr 3.644729e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 491 iter 22: train loss 1.53649. lr 3.598983e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 492 iter 22: train loss 1.50368. lr 3.553092e-03: 100% 23/23 [00:00<00:00, 30.67it/s]\n","epoch 493 iter 22: train loss 1.51269. lr 3.507066e-03: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 494 iter 22: train loss 1.53634. lr 3.460917e-03: 100% 23/23 [00:00<00:00, 32.25it/s]\n","epoch 495 iter 22: train loss 1.48552. lr 3.414656e-03: 100% 23/23 [00:00<00:00, 32.76it/s]\n","epoch 496 iter 22: train loss 1.51517. lr 3.368294e-03: 100% 23/23 [00:00<00:00, 32.90it/s]\n","epoch 497 iter 22: train loss 1.47608. lr 3.321843e-03: 100% 23/23 [00:00<00:00, 32.61it/s]\n","epoch 498 iter 22: train loss 1.50184. lr 3.275313e-03: 100% 23/23 [00:00<00:00, 32.54it/s]\n","epoch 499 iter 22: train loss 1.46357. lr 3.228717e-03: 100% 23/23 [00:00<00:00, 32.64it/s]\n","epoch 500 iter 22: train loss 1.50038. lr 3.182065e-03: 100% 23/23 [00:00<00:00, 32.80it/s]\n","epoch 501 iter 22: train loss 1.43646. lr 3.135369e-03: 100% 23/23 [00:00<00:00, 32.89it/s]\n","epoch 502 iter 22: train loss 1.53589. lr 3.088640e-03: 100% 23/23 [00:00<00:00, 32.59it/s]\n","epoch 503 iter 22: train loss 1.45539. lr 3.041889e-03: 100% 23/23 [00:00<00:00, 32.67it/s]\n","epoch 504 iter 22: train loss 1.46689. lr 2.995129e-03: 100% 23/23 [00:00<00:00, 32.83it/s]\n","epoch 505 iter 22: train loss 1.55799. lr 2.948369e-03: 100% 23/23 [00:00<00:00, 32.73it/s]\n","epoch 506 iter 22: train loss 1.49350. lr 2.901622e-03: 100% 23/23 [00:00<00:00, 30.95it/s]\n","epoch 507 iter 22: train loss 1.43976. lr 2.854899e-03: 100% 23/23 [00:00<00:00, 30.98it/s]\n","epoch 508 iter 22: train loss 1.48121. lr 2.808211e-03: 100% 23/23 [00:00<00:00, 28.51it/s]\n","epoch 509 iter 22: train loss 1.48002. lr 2.761570e-03: 100% 23/23 [00:00<00:00, 30.92it/s]\n","epoch 510 iter 22: train loss 1.43831. lr 2.714987e-03: 100% 23/23 [00:00<00:00, 31.53it/s]\n","epoch 511 iter 22: train loss 1.44246. lr 2.668473e-03: 100% 23/23 [00:00<00:00, 31.55it/s]\n","epoch 512 iter 22: train loss 1.45498. lr 2.622039e-03: 100% 23/23 [00:00<00:00, 31.78it/s]\n","epoch 513 iter 22: train loss 1.47419. lr 2.575697e-03: 100% 23/23 [00:00<00:00, 31.53it/s]\n","epoch 514 iter 22: train loss 1.50892. lr 2.529459e-03: 100% 23/23 [00:00<00:00, 32.00it/s]\n","epoch 515 iter 22: train loss 1.46178. lr 2.483334e-03: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 516 iter 22: train loss 1.42120. lr 2.437336e-03: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 517 iter 22: train loss 1.41435. lr 2.391474e-03: 100% 23/23 [00:00<00:00, 31.32it/s]\n","epoch 518 iter 22: train loss 1.40864. lr 2.345759e-03: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 519 iter 22: train loss 1.43303. lr 2.300204e-03: 100% 23/23 [00:00<00:00, 31.63it/s]\n","epoch 520 iter 22: train loss 1.46255. lr 2.254819e-03: 100% 23/23 [00:00<00:00, 31.78it/s]\n","epoch 521 iter 22: train loss 1.47621. lr 2.209615e-03: 100% 23/23 [00:00<00:00, 31.69it/s]\n","epoch 522 iter 22: train loss 1.46587. lr 2.164603e-03: 100% 23/23 [00:00<00:00, 31.58it/s]\n","epoch 523 iter 22: train loss 1.46965. lr 2.119793e-03: 100% 23/23 [00:00<00:00, 31.16it/s]\n","epoch 524 iter 22: train loss 1.43170. lr 2.075198e-03: 100% 23/23 [00:00<00:00, 31.72it/s]\n","epoch 525 iter 22: train loss 1.45555. lr 2.030827e-03: 100% 23/23 [00:00<00:00, 31.28it/s]\n","epoch 526 iter 22: train loss 1.45283. lr 1.986692e-03: 100% 23/23 [00:00<00:00, 31.65it/s]\n","epoch 527 iter 22: train loss 1.39981. lr 1.942803e-03: 100% 23/23 [00:00<00:00, 32.16it/s]\n","epoch 528 iter 22: train loss 1.40377. lr 1.899171e-03: 100% 23/23 [00:00<00:00, 32.03it/s]\n","epoch 529 iter 22: train loss 1.41261. lr 1.855807e-03: 100% 23/23 [00:00<00:00, 31.44it/s]\n","epoch 530 iter 22: train loss 1.40916. lr 1.812720e-03: 100% 23/23 [00:00<00:00, 31.97it/s]\n","epoch 531 iter 22: train loss 1.37445. lr 1.769922e-03: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 532 iter 22: train loss 1.36805. lr 1.727422e-03: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 533 iter 22: train loss 1.37939. lr 1.685232e-03: 100% 23/23 [00:00<00:00, 31.35it/s]\n","epoch 534 iter 22: train loss 1.33485. lr 1.643361e-03: 100% 23/23 [00:00<00:00, 31.91it/s]\n","epoch 535 iter 22: train loss 1.42006. lr 1.601820e-03: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 536 iter 22: train loss 1.40602. lr 1.560619e-03: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 537 iter 22: train loss 1.37238. lr 1.519767e-03: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 538 iter 22: train loss 1.33374. lr 1.479275e-03: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 539 iter 22: train loss 1.40139. lr 1.439153e-03: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 540 iter 22: train loss 1.39385. lr 1.399409e-03: 100% 23/23 [00:00<00:00, 31.58it/s]\n","epoch 541 iter 22: train loss 1.38716. lr 1.360055e-03: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 542 iter 22: train loss 1.34626. lr 1.321099e-03: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 543 iter 22: train loss 1.37144. lr 1.282551e-03: 100% 23/23 [00:00<00:00, 31.68it/s]\n","epoch 544 iter 22: train loss 1.33580. lr 1.244420e-03: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 545 iter 22: train loss 1.38803. lr 1.206716e-03: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 546 iter 22: train loss 1.34644. lr 1.169447e-03: 100% 23/23 [00:00<00:00, 31.95it/s]\n","epoch 547 iter 22: train loss 1.34130. lr 1.132623e-03: 100% 23/23 [00:00<00:00, 32.03it/s]\n","epoch 548 iter 22: train loss 1.34431. lr 1.096253e-03: 100% 23/23 [00:00<00:00, 32.12it/s]\n","epoch 549 iter 22: train loss 1.34665. lr 1.060345e-03: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 550 iter 22: train loss 1.32496. lr 1.024909e-03: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 551 iter 22: train loss 1.28773. lr 9.899527e-04: 100% 23/23 [00:00<00:00, 31.74it/s]\n","epoch 552 iter 22: train loss 1.34980. lr 9.554845e-04: 100% 23/23 [00:00<00:00, 32.36it/s]\n","epoch 553 iter 22: train loss 1.30890. lr 9.215132e-04: 100% 23/23 [00:00<00:00, 32.42it/s]\n","epoch 554 iter 22: train loss 1.28677. lr 8.880468e-04: 100% 23/23 [00:00<00:00, 32.20it/s]\n","epoch 555 iter 22: train loss 1.31707. lr 8.550935e-04: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 556 iter 22: train loss 1.32705. lr 8.226614e-04: 100% 23/23 [00:00<00:00, 32.06it/s]\n","epoch 557 iter 22: train loss 1.30328. lr 7.907583e-04: 100% 23/23 [00:00<00:00, 31.63it/s]\n","epoch 558 iter 22: train loss 1.34454. lr 7.593919e-04: 100% 23/23 [00:00<00:00, 32.08it/s]\n","epoch 559 iter 22: train loss 1.38945. lr 7.285699e-04: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 560 iter 22: train loss 1.35608. lr 6.982998e-04: 100% 23/23 [00:00<00:00, 32.20it/s]\n","epoch 561 iter 22: train loss 1.30356. lr 6.685889e-04: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 562 iter 22: train loss 1.33182. lr 6.394445e-04: 100% 23/23 [00:00<00:00, 32.15it/s]\n","epoch 563 iter 22: train loss 1.31684. lr 6.108735e-04: 100% 23/23 [00:00<00:00, 32.50it/s]\n","epoch 564 iter 22: train loss 1.28391. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 565 iter 22: train loss 1.30007. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 566 iter 22: train loss 1.30769. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 567 iter 22: train loss 1.25598. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.54it/s]\n","epoch 568 iter 22: train loss 1.33241. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.67it/s]\n","epoch 569 iter 22: train loss 1.27829. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.97it/s]\n","epoch 570 iter 22: train loss 1.30957. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.91it/s]\n","epoch 571 iter 22: train loss 1.29472. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.82it/s]\n","epoch 572 iter 22: train loss 1.33079. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.98it/s]\n","epoch 573 iter 22: train loss 1.31274. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.61it/s]\n","epoch 574 iter 22: train loss 1.27976. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.53it/s]\n","epoch 575 iter 22: train loss 1.35103. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.71it/s]\n","epoch 576 iter 22: train loss 1.33976. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.29it/s]\n","epoch 577 iter 22: train loss 1.32307. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.32it/s]\n","epoch 578 iter 22: train loss 1.26617. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.13it/s]\n","epoch 579 iter 22: train loss 1.30252. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.21it/s]\n","epoch 580 iter 22: train loss 1.24513. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.30it/s]\n","epoch 581 iter 22: train loss 1.31574. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.39it/s]\n","epoch 582 iter 22: train loss 1.25729. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.54it/s]\n","epoch 583 iter 22: train loss 1.29049. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.83it/s]\n","epoch 584 iter 22: train loss 1.21397. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.13it/s]\n","epoch 585 iter 22: train loss 1.24613. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.88it/s]\n","epoch 586 iter 22: train loss 1.25955. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.01it/s]\n","epoch 587 iter 22: train loss 1.31073. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 588 iter 22: train loss 1.24545. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 589 iter 22: train loss 1.32169. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.78it/s]\n","epoch 590 iter 22: train loss 1.25177. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.20it/s]\n","epoch 591 iter 22: train loss 1.29041. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.45it/s]\n","epoch 592 iter 22: train loss 1.25952. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.58it/s]\n","epoch 593 iter 22: train loss 1.26126. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.23it/s]\n","epoch 594 iter 22: train loss 1.29015. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.95it/s]\n","epoch 595 iter 22: train loss 1.35315. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 596 iter 22: train loss 1.27460. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 597 iter 22: train loss 1.27867. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.71it/s]\n","epoch 598 iter 22: train loss 1.28523. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.33it/s]\n","epoch 599 iter 22: train loss 1.32541. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.09it/s]\n","epoch 600 iter 22: train loss 1.30477. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.87it/s]\n","epoch 601 iter 22: train loss 1.35114. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.59it/s]\n","epoch 602 iter 22: train loss 1.28801. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.68it/s]\n","epoch 603 iter 22: train loss 1.29442. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.98it/s]\n","epoch 604 iter 22: train loss 1.25345. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.97it/s]\n","epoch 605 iter 22: train loss 1.28414. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.62it/s]\n","epoch 606 iter 22: train loss 1.25283. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 607 iter 22: train loss 1.33880. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 608 iter 22: train loss 1.25758. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.07it/s]\n","epoch 609 iter 22: train loss 1.24655. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 610 iter 22: train loss 1.34053. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.60it/s]\n","epoch 611 iter 22: train loss 1.29108. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.19it/s]\n","epoch 612 iter 22: train loss 1.28636. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.92it/s]\n","epoch 613 iter 22: train loss 1.26978. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.02it/s]\n","epoch 614 iter 22: train loss 1.26508. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 30.43it/s]\n","epoch 615 iter 22: train loss 1.29937. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.20it/s]\n","epoch 616 iter 22: train loss 1.22900. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.81it/s]\n","epoch 617 iter 22: train loss 1.29917. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.75it/s]\n","epoch 618 iter 22: train loss 1.23921. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.84it/s]\n","epoch 619 iter 22: train loss 1.34010. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.94it/s]\n","epoch 620 iter 22: train loss 1.30809. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.86it/s]\n","epoch 621 iter 22: train loss 1.30850. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.55it/s]\n","epoch 622 iter 22: train loss 1.24580. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.43it/s]\n","epoch 623 iter 22: train loss 1.30171. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 624 iter 22: train loss 1.26082. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.10it/s]\n","epoch 625 iter 22: train loss 1.24619. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.89it/s]\n","epoch 626 iter 22: train loss 1.26524. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.58it/s]\n","epoch 627 iter 22: train loss 1.21174. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.26it/s]\n","epoch 628 iter 22: train loss 1.23696. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.77it/s]\n","epoch 629 iter 22: train loss 1.28206. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.85it/s]\n","epoch 630 iter 22: train loss 1.27551. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.14it/s]\n","epoch 631 iter 22: train loss 1.33656. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.18it/s]\n","epoch 632 iter 22: train loss 1.30226. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.07it/s]\n","epoch 633 iter 22: train loss 1.26653. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.61it/s]\n","epoch 634 iter 22: train loss 1.24835. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.11it/s]\n","epoch 635 iter 22: train loss 1.32601. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.80it/s]\n","epoch 636 iter 22: train loss 1.26315. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.76it/s]\n","epoch 637 iter 22: train loss 1.30467. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.71it/s]\n","epoch 638 iter 22: train loss 1.30583. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.05it/s]\n","epoch 639 iter 22: train loss 1.31991. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.82it/s]\n","epoch 640 iter 22: train loss 1.27041. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.86it/s]\n","epoch 641 iter 22: train loss 1.23282. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.63it/s]\n","epoch 642 iter 22: train loss 1.29484. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.38it/s]\n","epoch 643 iter 22: train loss 1.24255. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 32.29it/s]\n","epoch 644 iter 22: train loss 1.29095. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.73it/s]\n","epoch 645 iter 22: train loss 1.29261. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 31.39it/s]\n","epoch 646 iter 22: train loss 1.25182. lr 6.013192e-04: 100% 23/23 [00:00<00:00, 32.54it/s]\n","epoch 647 iter 22: train loss 1.32166. lr 6.296941e-04: 100% 23/23 [00:00<00:00, 32.48it/s]\n","epoch 648 iter 22: train loss 1.22227. lr 6.586449e-04: 100% 23/23 [00:00<00:00, 32.75it/s]\n","epoch 649 iter 22: train loss 1.27472. lr 6.881646e-04: 100% 23/23 [00:00<00:00, 31.28it/s]\n","epoch 650 iter 22: train loss 1.28603. lr 7.182460e-04: 100% 23/23 [00:00<00:00, 30.55it/s]\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh perceiver_finetune_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UzyBZM84abI","executionInfo":{"status":"ok","timestamp":1732599128374,"user_tz":480,"elapsed":11769,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"71fb6738-2240-4071-c594-efc3772412e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:31:58.518108: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:31:58.538551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:31:58.560950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:31:58.567295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:31:58.582958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:31:59.729735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","epoch 1 iter 7: train loss 2.08475. lr 5.999844e-04: 100% 8/8 [00:00<00:00, 10.76it/s]\n","epoch 2 iter 7: train loss 1.89157. lr 5.999351e-04: 100% 8/8 [00:00<00:00, 18.50it/s]\n","epoch 3 iter 7: train loss 1.72003. lr 5.998520e-04: 100% 8/8 [00:00<00:00, 18.45it/s]\n","epoch 4 iter 7: train loss 1.62527. lr 5.997351e-04: 100% 8/8 [00:00<00:00, 18.46it/s]\n","epoch 5 iter 7: train loss 1.53140. lr 5.995844e-04: 100% 8/8 [00:00<00:00, 18.27it/s]\n","epoch 6 iter 7: train loss 1.44020. lr 5.993999e-04: 100% 8/8 [00:00<00:00, 17.95it/s]\n","epoch 7 iter 7: train loss 1.37257. lr 5.991818e-04: 100% 8/8 [00:00<00:00, 17.95it/s]\n","epoch 8 iter 7: train loss 1.29172. lr 5.989299e-04: 100% 8/8 [00:00<00:00, 17.89it/s]\n","epoch 9 iter 7: train loss 1.20508. lr 5.986444e-04: 100% 8/8 [00:00<00:00, 17.86it/s]\n","epoch 10 iter 7: train loss 1.10395. lr 5.983252e-04: 100% 8/8 [00:00<00:00, 17.68it/s]\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh perceiver_eval_dev_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpvp3cRT44rD","executionInfo":{"status":"ok","timestamp":1732599195524,"user_tz":480,"elapsed":67153,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"e1938e73-ecde-43d3-a3ae-abcd8bad3996"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:32:10.313055: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:32:10.331160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:32:10.352243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:32:10.359017: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:32:10.374871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:32:11.549669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:00,  8.21it/s]\n","Correct: 10.0 out of 500.0: 2.0%\n"]}]},{"cell_type":"code","source":["!cd src && bash run.sh perceiver_eval_test_with_pretrain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85gFqucu44us","executionInfo":{"status":"ok","timestamp":1732599255667,"user_tz":480,"elapsed":60148,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"}},"outputId":"bd51cd68-6c24-4bd7-b050-bdb45c2966db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-26 05:33:17.460493: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-26 05:33:17.479191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-26 05:33:17.500518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-26 05:33:17.506905: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-26 05:33:17.522091: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-26 05:33:18.704258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:53,  8.12it/s]\n","!!! No ground truth is provided, this will be done on the autograder, returning (0,0)\n","Predictions written to ./submission/perceiver.pretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3111,"status":"ok","timestamp":1732599289236,"user":{"displayName":"Xin Ju (Isaac)","userId":"07431887039181781801"},"user_tz":480},"id":"Ye8exJAJqb8B","outputId":"bae859cc-032d-404d-9bba-aae86934f063"},"outputs":[{"output_type":"stream","name":"stdout","text":["========== START GRADING\n","----- START 1c-0-basic:  check CharCorruptionDataset truncation length\n","data has 2768 characters, 26 unique.\n","----- END 1c-0-basic [took 0:00:00.036481 (max allowed 5 seconds), 5.0/5.0 points]\n","\n","----- START 1c-1-basic:  check CharCorruptionDataset rearrange\n","data has 2593 characters, 26 unique.\n","----- END 1c-1-basic [took 0:00:00.013318 (max allowed 5 seconds), 5.0/5.0 points]\n","\n","----- START 1c-2-basic:  check CharCorruptionDataset io\n","data has 2671 characters, 26 unique.\n","----- END 1c-2-basic [took 0:00:00.021763 (max allowed 5 seconds), 5.0/5.0 points]\n","\n","----- START 1c-3-basic:  check CharCorruptionDataset masked content length\n","data has 2701 characters, 26 unique.\n","----- END 1c-3-basic [took 0:00:00.013756 (max allowed 5 seconds), 5.0/5.0 points]\n","\n","----- START 1d-0-hidden:  vanilla model similarity\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","number of parameters: 3323392\n","----- END 1d-0-hidden [took 0:00:00.057694 (max allowed 5 seconds), ???/2.0 points] (hidden test ungraded)\n","\n","----- START 1d-1-basic:  correct trainer object initialization for finetune without pretraining\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","----- END 1d-1-basic [took 0:00:00.128366 (max allowed 15 seconds), 2.0/2.0 points]\n","\n","----- START 1e-0-hidden:   test the dev score for vanilla attention without pretrain\n","----- END 1e-0-hidden [took 0:00:00.004569 (max allowed 5 seconds), ???/2.0 points] (hidden test ungraded)\n","\n","----- START 1e-1-hidden:   test the test score for vanilla attention without pretrain\n","----- END 1e-1-hidden [took 0:00:00.003240 (max allowed 5 seconds), ???/4.0 points] (hidden test ungraded)\n","\n","----- START 1f-0-basic:  check basic vanilla pretrain trainer object\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","use device: 0\n","----- END 1f-0-basic [took 0:00:00.008758 (max allowed 5 seconds), 7.0/7.0 points]\n","\n","----- START 1f-1-hidden:   test the dev score for vanilla attention with pretrain\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","----- END 1f-1-hidden [took 0:00:00.004944 (max allowed 5 seconds), ???/5.0 points] (hidden test ungraded)\n","\n","----- START 1f-2-hidden:   test the test score for vanilla attention with pretrain\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","----- END 1f-2-hidden [took 0:00:00.004995 (max allowed 5 seconds), ???/8.0 points] (hidden test ungraded)\n","\n","----- START 1g-0-hidden:   test the dev score for perceiver attention with pretrain\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","<class 'AssertionError'>\n","0.02 not greater than or equal to 0.022\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 59, in testPartExecutor\n","    yield\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 591, in run\n","    self._callTestMethod(testMethod)\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 549, in _callTestMethod\n","    method()\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/graderUtil.py\", line 54, in wrapper\n","    result = func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/graderUtil.py\", line 84, in wrapper\n","    result = func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/grader.py\", line 241, in test_1\n","    self.assertGreaterEqual(n_correct / n_total, 0.022)\n","----- END 1g-0-hidden [took 0:00:00.007260 (max allowed 5 seconds), ???/7.0 points] (hidden test ungraded)\n","\n","----- START 1g-1-hidden:   test the test score for perceiver attention with pretrain\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","<class 'AssertionError'>\n","0.011441647597254004 not greater than or equal to 0.016\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 59, in testPartExecutor\n","    yield\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 591, in run\n","    self._callTestMethod(testMethod)\n","  File \"/usr/lib/python3.10/unittest/case.py\", line 549, in _callTestMethod\n","    method()\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/graderUtil.py\", line 54, in wrapper\n","    result = func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/graderUtil.py\", line 84, in wrapper\n","    result = func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MyCourse/02_stanford/01_CS/03_NLP/01_X224N/Week5_materials/src/grader.py\", line 249, in test_2\n","    self.assertGreaterEqual(n_correct / n_total, 0.016)\n","----- END 1g-1-hidden [took 0:00:00.018219 (max allowed 5 seconds), ???/7.0 points] (hidden test ungraded)\n","\n","Note that the hidden test cases do not check for correctness.\n","They are provided for you to verify that the functions do not crash and run within the time limit.\n","Points for these parts not assigned by the grader unless the solution is present (indicated by \"???\").\n","========== END GRADING [29.0/29.0 points + 0/0 extra credit]\n"]}],"source":["!cd src && python grader.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9grcNnIjzszf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}